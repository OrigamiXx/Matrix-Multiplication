\documentclass[11pt]{article}
\usepackage{fullpage}
\usepackage{rpmacros}
\RequirePackage[colorlinks=true]{hyperref}
\hypersetup{
  linkcolor=[rgb]{0,0,0.4},
  citecolor=[rgb]{0, 0.4, 0},
  urlcolor=[rgb]{0.6, 0, 0}
}
\usepackage{mathpazo}
\usepackage{bbm}
\usepackage{todonotes}
\usepackage{lipsum}
\usepackage{setspace}
\usepackage{mdframed}
\usepackage{multicol}
\usepackage{graphicx}
\usepackage{comment}
\usepackage{tikz}
\usetikzlibrary{decorations.pathreplacing,backgrounds}

\usepackage[font=footnotesize]{caption}

\input{thmmacros}
\input{customurlbst/bibmacros}

\usepackage{algorithmicx}
\usepackage{algorithm} % http://ctan.org/pkg/algorithms
\usepackage[noend]{algpseudocode}
\renewcommand{\algorithmicensure}{\textbf{Output:}}
\renewcommand{\algorithmicrequire}{\textbf{Input:}}


\algrenewcommand\algorithmicindent{1.0em}%

\newcommand{\RPnote}[1]{\textcolor{BrickRed}{RP: #1}}
\newcommand{\Mattnote}[1]{\textcolor{OliveGreen}{MWA: #1}}
\newcommand{\BLnote}[1]{\textcolor{Blue}{BLV: #1}}
\newcommand{\Anote}[1]{\textcolor{Plum}{A: #1}}
\newcommand{\MFnote}[1]{\textcolor{DarkOrchid}{MF: #1}}

\newcommand*\samethanks[1][\value{footnote}]{\footnotemark[#1]}
\newcommand\sse{\subseteq}
\newcommand\Sym[1]{\ensuremath{\mathrm{Sym}_{#1}}}
\newcommand\condset[2]{\set{#1 \;|\; #2}}
\renewcommand\NP{\ensuremath{\mathsf{NP}}}
\newcommand\coNP{\ensuremath{\mathsf{coNP}}}

%\onehalfspacing
\date{}

\title{Matrix Multiplication: Finding Strong Uniquely-Solvable Puzzles
{\IfFileExists{./sha.tex}{\\\small SHA: \input{sha}}{}}}
\author{
Matthew Anderson\thanks{Department of Computer Science, Union College, Schenectady, New York, USA, E-mails: \texttt{andersm2@union.edu, jiz@union.edu, xua@union.edu}}%
\and%
Zongliang Ji\samethanks[1]
\and%
Anthony Yang Xu\samethanks[1]
}
\begin{document}
\maketitle

\begin{abstract}

\end{abstract}

\thispagestyle{empty}
\newpage
\pagenumbering{arabic}


\section{Introduction}
\label{sec:intro}

% Context

An optimal algorithm for matrix multiplication is remains elusive
despite substantial effort.  We focus on the square variant of the
matrix multiplication problem, that is, given two $n$-by-$n$ matrices
$A$ and $B$ over a field $\F$, the goal is to compute the matrix
product $C = A \times B$.  The question that arised is: How many field
operations are required to compute $C$.  The na\"{i}ve algorithm,
based on the mathematical definition of matrix product, runs in time
$O(n^3)$, and for a time it was thought to be the optimal algorithm.
It was surprising when Strassen showed that matrix multiplication can
be done in time $O(n^{2.808})$ \cite{str69}, using a
divide-and-conquer approach.  A long sequence of work concluding with
Coppersmith and Winograd's laser method that reduced the running time
to $O(2^{2.376})$ \cite{pan78,b79,sch81,cw82,str86,cw87}. Recent
computer-aided refinements of Coppersmith and Winograd's work done by
Stothers, Vassilevska-Williams, and Le Gall further reduced the
exponent to $\omega \le 2.3728639$ \cite{sto10,vas11,leg14}.

There are some lower bounds on the time complexity of matrix
multiplication.  Na\"{i}ely, the dimensions of the output matrix $C$
imply that the problem requires at least $\Omega(n^2)$ time.  Slightly
better lower bounds are also known for specialized models of
computation, like bounded-depth arithmetic circuits \cite{XXX}.  There
are also lower bounds known for a variety of algorithmic approaches
for matrix multiplication.  Ambainis et al showed that the laser
method, the approach of Coppersmith-Winograd and subsequent
refinements, cannot alone achieve an algorithm with $\omega \le
2.3078$ running time \cite{afl14}.

% Other lower bounds?
% Tensor rank approach
% - Lower bounds
% - Impossibility

% Motivation

Cohn and Umans \cite{cu03} introduced a program for developing faster
algorithms for matrix multiplication by reducing this question to a
search for groups with subsets that satisfy a certain algebraic
property called the \emph{triple-product property} that allows matrix
multiplication to be embed in the group algebra.  Subsequent work
\cite{cksu05} elaborated on this idea and developed the notion of
\emph{strong uniquely solvable puzzles} (SUSP) whose existence would
also imply faster matrix multiplication algorithms.

A \emph{width}-$k$ puzzle $P$ is a subset of $U_k = \set{0,1,2}^k$,
and the cardinality of $P$ is the puzzle's \emph{size}.  Each element
of $P$ is called a \emph{row} of $P$, and each row consists of three
\emph{subrows} which are elements of $\set{0,*}^k$, $\set{1,*}^k$,
$\set{2,*}^k$ respectively.  Informally, a puzzle $P$ is a uniquely
solvable puzzle if there is no way to permute the subrows of $P$
without overlap to form a distinct puzzle $P'$.  A uniquely solvable
puzzle is \emph{strong} if stronger condition for non-overlapping is
applied. For a fixed width $k$, the larger the size of a puzzle $P$,
the faster matrix multiplication algorithm it implies if $P$ is a SUSP
\cite[Corollary 3.6]{cksu05}.  In fact Cohn et al.~show that there
exist an infinite family of SUSP that achieve $w < 2.48$
\cite[Proposition 3.8]{cksu05}.

% Approach

We follow Cohn and Umans' program simulateneously from theoretical and
experimental perspectives.  We explore properties of SUSP, develop
\emph{verification} algorithms for determining whether a puzzle is a
SUSP, develop \emph{search} algorithms for searching for large SUSP,
and implement these algorithms in desktop and high performance
computing settings.  From the computational complexity perspective the
algorithms we develop to verify and search for SUSP are not efficient
as they run in exponential or doubly exponential time in the natural
parameters.  However, as the goal is to find a sufficiently large SUSP
which in turn produces a fast matrix multiplication algorithm, the
inefficiency of our algorithms does not directly impact the efficiency
of the matrix multiplication algorithms.  Rather, it indirectly
impacts the efficiency by limiting the search space of puzzles that we
can practically examine.

% Results

In addition to the various theoretical results, we have experimental
results that bound the size of the largest SUSP for small width
puzzles.  For small constant width the bounds are tight, though the
tightness of the results decreases as width increases.  Lower bounds
on size are witnessed by examples of SUSP we found via search and
constructions that compose SUSP of small width into SUSP of larger
width while maintaining the relative size of the SUSP.  Upper bounds
are determined computationally by evaluating admissible heuristics on
the initial levels of the puzzle search tree.  Although our current
experimental results do not beat Proposition 3.8 of \cite{cksu05} for
unbounded $k$, they do suggest there is considerable room for
improvement.

% Organization
\paragraph{Organization}
We begin with formal definitions and properties of puzzles and strong
uniquely solvable puzzles in \autoref{sec:prelim}.  In
\autoref{sec:verify} we discuss our algorithms for verifying that a
puzzle is a SUSP.  In \autoref{sec:heuristic} we describe fast,
but incomplete, heuristics for verifying SUSP.
\autoref{sec:search} explains our search algorithms.
\autoref{sec:results} discuss our implementation, experiments, and
the experimental results.


\section{Preliminaries}
\label{sec:prelim}

\newcommand\ordset[1]{\ensuremath{[#1]}}

We use $\ordset{n}$ to denote the set $\set{0,1,2,\ldots, n-1}$.  For
a set $Q$, $\Sym{Q}$ denotes the symmetric group on the elements of
$Q$.  \cite{cksu05} introduced the idea of a \emph{puzzle}.

\begin{definition}[Puzzle]
  For $s, k \in \Natural$, an $(s,k)$-\emph{puzzle} is a
  subset $P \sse U_k = \ordset{3}^k$ with $|P| = s$.
\end{definition}

We say that an $(s,k)$-puzzle has $s$ rows and $k$ columns.  The
columns are inherent ordered and indexed by $\ordset{k}$.  The rows are not
inherently ordered, however, it is often convienent to assume that the
rows are arbitrarily ordered and indexed by $\ordset{s}$.

\cite{cksu05} also establish a particular combinatorial property of
such puzzles that can derive groups that matrix multiplication can be
embedded into.  Such puzzles are called \emph{strong} uniquely
solvable puzzles.  However, to give some intuition we first explain a
simpler version of the property called \emph{uniquely solvable
  puzzles}.

\begin{definition}[Uniquely Solvable Puzzle (USP)]
  \label{def:strong-USP}
  ~\\An $(s,k)$-puzzle $P$ is \emph{uniquely solvable} if
  $\forall \pi_0, \pi_1, \pi_2 \in \Sym{P}:$
  \begin{enumerate}
  \item either $\pi_0 = \pi_1 = \pi_2$, or
  \item $\exists r \in P, \exists i \in \ordset{k}$ such that at least two
    of the following hold:
    \begin{enumerate}
    \item $(\pi_0(r))_i = 0$,
    \item $(\pi_1(r))_i = 1$,
    \item $(\pi_2(r))_i = 2$.
    \end{enumerate}
  \end{enumerate}
\end{definition}

Informally a puzzle is \textbf{not} uniquely solvable if each row of
the puzzle can be broken into zeros, ones, and twos pieces and then
the rows can be reassembled in a different way so that each new row is
a combination of a zeroes, a ones, and twos piece where there is
exactly element of $\ordset{3}$ for each column.  Observe that uniquely
solvable puzzles can have at most $2^k$ rows because each zeroes
piece, ones piece, and two piece must be unique, as otherwise the
duplicate pieces can be swapped making the puzzle not uniquely
solvable.  The definition of \emph{strong} uniquely solvable puzzle is
below, it is nearly the same except that it requires that there be a
collision on a column between exactly two pieces, not two or more
pieces like in the original definition.

\begin{definition}[Strong Uniquely Solvable Puzzle]
  ~\\
  An $(s,k)$-puzzle $P$ is \emph{strong uniquely solvable} if
  $\forall \pi_0, \pi_1, \pi_2 \in \Sym{P}:$
  \begin{enumerate}
  \item either $\pi_0 = \pi_1 = \pi_2$, or
  \item $\exists r \in P, \exists i \in \ordset{k}$ such that exactly two
    of the following hold:
    \begin{enumerate}
    \item $(\pi_0(r))_i = 0$,
    \item $(\pi_1(r))_i = 1$,
    \item $(\pi_2(r))_i = 2$.
    \end{enumerate}
  \end{enumerate}
  
\end{definition}

Observe that the properties of uniquely solvable and strong uniquely
solvable are invariant to the ordering of the rows or columns of a
puzzle.  This fact is used implicitly.  Also note that these
properties are invariant to maps between puzzles induced by
permutations from \Sym{\ordset{3}} on the puzzle cell elements.

Cohn et al.~ show the following connection between the existence of
strong uniquely solvable puzzles and upper bounds on the exponent of
matrix multiplication $\omega$.

\begin{lemma}[{\cite[Corollary 3.6]{cksu05}}]
  If there is a strong uniquely solvable $(s,k)$-puzzle,
  $$\omega \le \min_{m \ge 3, m \in \Natural} \frac{3 \log
    m}{\log(m-1)} - \frac{3 \log s!}{sk \log(m-1)}.$$
\end{lemma}

In the same article, the authors also demonstrate the existence of an
infinite family of strong uniquely solvable puzzles that achieves a
non-trivial bound on $\omega$.

\begin{lemma}[{\cite[Proposition 3.8]{cksu05}}]
  There is an infinite family of strong uniquely solvable puzzles that
  achieves $\omega < 2.48$.
\end{lemma}

Finally, they conjecture that strong uniquely solvable puzzles provide
a root to achieving quadratic time matrix multiplication.

\begin{conjecture}[{\cite{cksu05}}]
  There exists a family of strong uniquely solvable puzzles that
  implies $\omega = 2$.
\end{conjecture}

Unfortunately, this conjecture was recently shown to be false.

\begin{lemma}[\cite{bccgu16}]
  Strong uniquely solvable puzzles cannot show $\omega < 2 +
  \epsilon$, for some $\epsilon > 0$.
\end{lemma}

This result is a consequence of a recent breakthrough arithmetic
progressions in cap sets \cite{e16,clp16} combined with a conditional
result on the Erd\"{o}s-Szemeredi sunflower conjecture \cite{asu13}.
The results of \cite{bccgu16} do imply that Cohn and Umans' strong
uniquely solvable puzzle approach cannot achieve the ideal $\omega =
2$.  However, we are unaware of a concrete lower bound on $\epsilon$
implies by this result.  This means there is a still a substantial gap
in our understanding between what has been acheived by the refinements
of LeGall, Williams, and Stothers, and the impossibility of showing
$\omega = 2$ using the Cohn and Umans' approach.



\section{Verifying Strong USPs}
\label{sec:verify}

The core focus of this article is the verification of strong
uniquely-solvable puzzles.  In particular, we are interested in the
decision problem: Given an $(s,k)$-puzzle $P$, output YES iff $P$ is a
strong uniquely-solvable puzzle.  In this section we discuss the
design of efficient and practical algorithms to solve this problem as
a function of the parameters $s$ and $k$.  As the goal of this work is
to use computers to locate large strong USPs, we also discuss aspects
our implementation that informed or constrained our designs.  All of
the exact algorithms we develop in this section have exponential
running time in the parameters $s$ and $k$.  However, although we will
discuss the asymptotic worst-case running time of our algorithms, this
is not the metric we are truly interested in.  Rather we are
interested in the practical performance of our algorithms and their
capability for locating new large strong USPs.

The algorithm that we ultimately develop and implement in this section
is a hybrid of a number of simpler algorithms and heuristics.  The
primary reason for this is that the algorithms with better asymptotic
and practical performance at larger input lengths have large overhead
at small input lengths.  Although we have two parameters $s$ and $k$
they are not fully independent.  First, $s \le 3^k$ because the
maximum number of rows in a puzzle of width $k$ is $|[3]^k| = 3^k$.
Second, we can eliminate the dependence on $k$ entirely by
transforming an $(s,k)$-puzzle into a 3D matching instance on the
vertex set $[s]^3$.  However, this transformation is not free because
the instance size is now a function of the cube of $s$ rather than
linear in $s$.

\subsection{Brute Force}

The obvious algorithm for verification comes directly from the
definition of strong uniquely solvable puzzles
(\autoref{def:strong-USP}). 

\begin{algorithm}
  \caption{: Brute Force}
  \label{alg:brute-force}
\begin{algorithmic}[1]
  \Require{An $(s,k)$-puzzle $P$.}
  \Ensure{YES, if $P$ is a strong USP and NO otherwise.}
  \Function{VerifyBruteForce}{$P$}
  \For{$\pi_1 \in \Sym{P}$}
    \For{$\pi_2 \in \Sym{P}$}
      \If{$\pi_1 \neq 1 \vee \pi_2 \neq 1$}
        \State{$found = false.$}
        \For{$r \in P$}
          \For{$i \in [k]$}
            \If{$\delta_{r_i, 0} + \delta_{(\pi_1(r))_i, 1} + \delta_{(\pi_2(r))_i, 2} = 2$} $found = true$. \EndIf
          \EndFor
        \EndFor
        \If{not $found$} \Return{NO.} \EndIf  
      \EndIf
    \EndFor
  \EndFor
  \State{\Return{YES}.}
  \EndFunction
\end{algorithmic}
\end{algorithm}
Note that $1$ in Line 3 denotes the identity permutation in $\Sym{P}$,
and $\delta_{a,b}$ is the Kronecker delta function which is $1$ if $a
= b$ and $0$ otherwise.  Observe that \autoref{alg:brute-force} does
not refer to $\pi_0$ of \autoref{def:strong-USP}.  This is because the
strong USP property is invariant to permutations of the rows and so
$\pi_0$ can be thought of as an arbitrary phase.  We fix $\pi_0 = 1$
to simplify the algorithm.  Noting that $|\Sym{P}| = s!$, the four
nested loops make the running time of this algorithm is easy to
analyze.  The algorithm runs in time $O((s!)^2 \cdot s \cdot k \cdot
\poly(s))$ where the last factor low polynomial factor of $s$ accounts
for the time to perform operations on permutations.  The dominant term
in the running time is the contribution from iterating over pairs of
permutations, though this term was made a factor $s!$ smaller by not
iterating over $\pi_0$.  Finally, notice that if $P$ is a strong USP,
then the algorithm runs in time $\Theta((s!)^2 \cdot k \cdot
\poly(s))$, and that if $P$ is not a strong USP the algorithm may
terminate early.  This algorithm's poor theoretical and practical
performance made it unusable in our implementation, however, it's
simplicity and direct connection to the definition made its
implementation a valuable sanity check against later more elaborate
algorithms (and served as effective onboarding to undergraduate
students helping with this project).

Although \autoref{alg:brute-force} performs poorly, examining the
structure of a seemingly trivial optimization leads to substantially
more effective algorithms.

Consider the following function on triples of rows $a, b,
c$ $$f(a,b,c) = \vee_{i \in [k]} (\delta_{a_i,0} + \delta_{b_i,1} +
\delta_{c_i,2} = 2).$$ We can replace the innermost loop in Lines 6 \&
7 of \autoref{alg:brute-force} with the statement $found = found \vee
f(r, \pi_1(r), \pi_2(r))$.  Observe that $f$ neither depends on $P$,
$r$, nor the permutations, and that \autoref{alg:brute-force} no
longer depends on $k$.  To slightly speed up \autoref{alg:brute-force}
we can precompute and cache $f$ before the algorithm starts and then
look up values as the algorithm runs.  There are two obvious options
to consider we can either precompute $f$ specialized to the rows in
the puzzle $P$, or we can precompute $f$ for all possible rows.  In
former case the time to precompute $f$ is $\Theta(s^3 \cdot k)$ and in
the later case $\Theta(3^{3k} \cdot k)$.  The storage requirements are
$\Theta(s^3)$ and $\Theta(3^{3k})$ bits respectively.  The former is
problematic for large $s$ and later problematic even for small $k$.
Moreover the combined running time for the two options with a single
call to verify a puzzle is $\Theta(s^3 \cdot k + (s!)^2 \cdot
\poly(s))$ and $\Theta(3^{3k} \cdot k + (s!)^2 \cdot \poly(s))$.  In
the former case there is an asymptotic improvement, but in the later
case, the saving of a factor of $k$ is easily offset by the additional
$3^{3k}$ term.  For this reason we rule out the later option and chose
to represent the function $f$ specialized to $f_P$ for a given puzzle
$P$.

\subsection{Verification to 3D Matching}

For verification it turns out to be more useful to work with $f_p$
than with $P$.  It is convienent to think of $f_P$ from several
perspectives: (i) as a function $f_P : P \times P \times P \rightarrow
\set{0, 1}$, (ii) as an order three tensor $f_p \in \set{0,1}^{P
  \times P \times P}$, and (iii) as the complement of the
characteristic function of hyperedge relations of a tripartite
hypergraph $H_P = \langle P \sqcup P \sqcup P, \bar{f_p}\rangle$ where
the vertex set is the disjoint union of three copies of $P$.  The last
of these, $H_P$, is the most useful.

Let $G = \langle P \sqcup P \sqcup P, E \sse P^3\rangle$ be a
tripartite 3-hypergraph.  We say that $G$ has a \emph{3D matching} iff
there exists a subset $M \sse E$ with $|M| = |P|$ and for all distinct
hyperedges $e_1, e_2 \in M$, $e_1$ and $e_2$ are \emph{vertex
  disjoint}, that is, $(e_1)_i \neq (e_2)_i$ for all $i \in [3]$.  We
say a 3D matching is \emph{nontrival} if it is not the matching
$\condset{(r,r,r)}{r \in P}$.

\begin{lemma}
  \label{lem:verify-to-3dm}
  An $(s,k)$-puzzle $P$ is a strong USP iff $H_P$ has no nontrivial 3D
  matching.
\end{lemma}

\begin{proof}
  We first argue the reverse direction.  Suppose that $H_p$ has a
  nontrivial 3D matching $M$.  We show that $P$ is not a strong USP by
  using $M$ to construct $\pi_0, \pi_1, \pi_2 \in \Sym{P}$ that
  witness this.  Let $\pi_0$ be the identity permutation.  For each $r
  \in P$, define $\pi_1(r) = q$ where $(r,q,\_) \in M$.  Note that $q$
  is well defined and unique because $M$ is 3D matching and has vertex
  disjoint edges.  Similarly define $\pi_2(r) = q$ where $(r,\_,q) \in
  M$.  Observe that by construction $M =
  \condset{(\pi_0(r),\pi_1(r),\pi_2(r))}{r \in P}$.  Since $M$ is a
  matching of $H_P$, $M \sse \bar{f_P}$.  Because $M$ is a nontrival
  matching at least one edge in $(a,b,c) \in M$ is has either $a \neq
  b$, $a \neq c$, or $b \neq c$.  This implies, respectively, that as
  constructed $\pi_0 \neq \pi_1$, $\pi_0 \neq \pi_2$, or $\pi_1 \neq
  \pi_2$.  In each case we have determined that $\pi_0$, $\pi_1$, and
  $\pi_2$ are not all identical.  Thus we determined permutations such
  that for all $r \in P$, $f(\pi_0(r), \pi_1(r), \pi_2(r)) = 0$.  This
  violates Condition 2 of \autoref{def:strong-USP}, hence $P$ is not a
  strong USP.

  The forward direction of the lemma is argued symmetrically.  Suppose
  that $P$ is not a strong USP. We show that $H_P$ has a 3D matching.
  For $P$ not to be a strong USP there must exist $\pi_0, \pi_1, \pi_2
  \in \Sym{P}$ not all identical such that Condition 2 of
  \autoref{def:strong-USP} fails.  Define $e(r) =
  (\pi_0(r),\pi_1(r),\pi_2(r))$ and $M = \condset{e(r)}{r \in P}$.
  Since Condition 2 fails, we have that $f_P(e(r)) = false$ for all $r
  \in P$.  This means that for all $r \in P$< $e(r) \in \bar{f_P}$ and
  hence $M \sse \bar{f_P}$.  Since $\pi_0$ is a permutation $|M| =
  |P|$.  Finally, observe that $M$ is nontrivial because not all of
  the permutations are identical and there must be some $r \in P$ with
  $e(r)$ having non identical coordinates.  Thus $M$ is a nontrivial
  3D matching of $H_p$.
\end{proof}

Although 3D matching is an \NP-complete problem \cite{karp72},
\autoref{lem:verify-to-3dm} does not immediately imply that
verification of strong USP is \coNP-complete because $H_P$ is not an
arbitrary hypergraph.  (That verification is in \coNP is immediate
from \autoref{def:strong-USP}.)  Indeed, it seems difficult to define a
puzzle $P$ that allows hyperedges in $H_P$ to be included
independently.  It remains open whether verification is
\coNP-complete, but our implementation and experimental data suggests
that it is likely not to be the case.

\autoref{lem:verify-to-3dm} implies that to verify $P$ is a strong USP
it suffices to determine whether $H_P$ has a 3D matching.  In the
subsequent sections we examine algorithms for the later problem.  We
can also view \autoref{alg:brute-force} as algorithm for solving 3D
matching.  Since we believe that verification is not \coNP-complete we
will use properties of strong USP and puzzles to provide additional
structure that algorithms can take advantage of.

XXX - Discuss simplifyTDM.

\subsection{Bidirectional Search}

The realization that verification of strong USP is a variant of 3D
matching quickly led to a linear exponential time algorithm which much
more practical.  The reduction allows us to replace the permutations
from $\Sym{P}$ with subsets of $P$ and effectively reduce the cost of
the outer loops of \autoref{alg:brute-force} from $s! =
\Theta(2^{s\log s}$ to $2^s$.  We now describe a recursive
bidirectional strong USP verification algorithm that uses this
observation.

\begin{algorithm}
  \caption{: Bidirectional}
  \label{alg:bi}
\begin{algorithmic}[1]
  \Require{An $(s,k)$-puzzle $P$.}
  \Ensure{YES, if $P$ is a strong USP and NO otherwise.}
  \Function{VerifyBidirectional}{$P$}
  \State{Let $T = \emptyset$.}
  \State{Construct $H_P$.}
  \Function{SearchHalf}{$\ell, Q,\ell_Q, R,\ell_R, \delta, t$}
  \If{$\ell = t$}
    \If{$\delta = 1$} \Comment{Forward Base Case}
      \State{Insert $(Q,R)$ into $T$.}
      \State{\Return{$false$}.}
    \Else \Comment{Reverse Base Case}
      \If{$(P-Q, P-R) \in T$} \State{\Return{$true$}.} \Else \State{\Return{$false$}.} \EndIf
    \EndIf
  \EndIf    
  \State{$result = false$.} \Comment{Recursive Case}
  \For{$\ell'_Q = \ell_Q + 1$ to $s$}
    \For{$\ell'_R = \ell_R + 1$ to $s$}
      \If{$(p_\ell, p_{\ell'_Q}, p_{\ell'_R}) \in H_P$}
        \State{$result = result~\vee$ \textsc{SearchHalf}$(\ell + \delta, Q \cup \set{p_{\ell'_Q}}, \ell'_Q, R \cup \set{p_{\ell'_R}}, \ell'_R, \delta, t)$.}
      \EndIf
    \EndFor
  \EndFor
  \State{\Return{$result$.}}
  \EndFunction
  
  \State{\textsc{SearchHalf}$(1,\emptyset, 0, \emptyset, 0, 1, \lfloor s / 2 \rfloor + 1)$.}
  \State{\Return{\textsc{SearchHalf}$(s, \emptyset, 0, \emptyset, 0, -1, \lfloor s/2 \rfloor)$}.}
  \EndFunction
\end{algorithmic}
\end{algorithm}

\autoref{alg:bi} consists of two phases. Let $t = s/2 \rfloor$. he
first phase determines all possible sets $Q,R \sse P$ with $|Q| = |R|
= t$ such that there is 3D matching $M_1$ of $H_P$ when restricted to
the vertices $\set{p_1,p_2, \ldots, p_t} \sqcup Q \sqcup R$.  The sets
$Q,R$ satisfying the requirement are stored in $T$ during the first
phase on Line 6.  The second phase determines all possible sets $Q,R
\sse P$ with $|Q| = |R| = s - t$ such that there is a 3D matching
$M_2$ of $H_P$ when restricted to the vertices
$\set{p_{t+1},p_{t+2},\ldots,p_s} \sqcup Q \sqcup R$.  For each pair
$(Q,R)$ the algorithm determines in the second phase it checks whether
$(P - Q, P - R)$ was inserted into $T$ during the first phase.  If the
pair is present it means that there is a 3D matching of $H_P$ which is
$M = M_1 \cup M_2$.  This works because $M_1$ and $M_2$ are partial 3D
matchings and vertex disjoint.  This is because $M_1$ is a 3D matching
on $\set{p_1,\ldots,p_t}$ and $M_2$ is a 3D matching on
$\set{p_{t+1},\ldots p_s}$, and because in Line 9 of \autoref{alg:bi}
the lookup is performed on the complementary sets.  The second phase
returns whether a complete matching could be found, and hence by
\autoref{lem:verify-to-3dm} whether $P$ is a strong USP.  (Note that
the first phase always returns $false$.)

The running time of this algorithm is dominanted by the number of
pairs of sets $(Q,R)$ it examines.  Observe that rows of $P$ are
considered in order in Lines 14 \& 15.  Further, the algorithm tracks
the index of last elements added to $Q$ and $R$ in $\ell_Q$ and
$\ell_R$ respectively.  The algorithm only adds new elements to $Q$ or
$R$ that have higher indexes than ones previously added.  Altogether
this implies that each pair of sets $(Q,R)$ is only considered at most
once during a phase.  Since $Q, R \sse P$, there are at most $2^s
\cdot 2^s$ pairs $(Q,R)$.  This means that \textsc{SearchHalf} is
called at most $4^s$ times during each phase.  Hence the running time
of the algorithm is $O(4^s \cdot s^2 \cdot \poly(s) + T_{3DM}(s,k))$
where $s^2$ factor comes from the inner loops and $T_{3DM}(s,k)$
accounts for the time to construct $H_P$.  The memory requirements of
\autoref{alg:bi} are similarly high -- the first phase uses $O(4^s
\cdot s)$ to store $T$.  

Note that algorithm does not early terminate on $P$ that are strong
USP, because it must search through all pairs before determining that
none can be found.  The algorithm could be modified to allow early
termination when $P$ is not a strong USP by causing the second phase
of search to immediately return in Line 17 once the first 3D matching
witness has been located.  However, this still requires the first
phase to run to completion.  A remedy for this would be to run both
phases in parallel and have them check against each other.  This would
substantially complicate the implementation, and not substantially
improve the practical running time for puzzles that are strong USP.

In the implementation of \autoref{alg:bi} we represented the sets
$Q,R$ using bit sets encoded into a single 64-bit long.  This
permitted it to represent these sets for $s \le 32$.  We implemented
$T$ using a hash table (maps from the C++ standard template library).
These choices made the basic data structure operations in
implementation effectively constant time, and have low memory overhead
beyond the contents of $T$.


\subsection{3DM to 3SAT}

XXX - Todo Jerry.

\label{subsec:sat}

\begin{algorithm}
  \caption{: Reduction to satisfiability}
  \label{alg:sat}
\begin{algorithmic}[1]
  \Require{An $(s,k)$-puzzle $P$.}
  \Ensure{YES, if $P$ is a strong USP and NO otherwise.}
  \Function{VerifySAT}{$P$}
  \EndFunction
\end{algorithmic}
\end{algorithm}
  
\subsection{3DM to MIP}

XXX - Todo Anthony.

\label{subsec:mip}

\begin{algorithm}
  \caption{: Reduction to mixed integer programming}
  \label{alg:mip}
\begin{algorithmic}[1]
  \Require{An $(s,k)$-puzzle $P$.}
  \Ensure{YES, if $P$ is a strong USP and NO otherwise.}
  \Function{VerifyMIP}{$P$}
  \EndFunction
\end{algorithmic}
\end{algorithm}


\subsection{Heuristics}
\label{sec:heuristic}

Despite our efforts devise an exact algorithm for verifying strong
USPs, the performance of the final algorithm was not practical.  To
speed the algorithm up we considered a number of verification
heuristics that were fast to evaluate, but did not alway produce a
definitive answer.  The heuristics output YES, NO, or MAYBE to whether
a given puzzle $P$ was a strong USP.  We consider only Las Vegas
algorithms here, that is, algorithms whose output of YES or NO is
always correct.  Further, all the heuristics we consider are one-sided
in that they for all input they either return YES or MAYBE, or NO or
MAYBE.  To verify a puzzle $P$ we run a battery of fast heuristics and
return early if any of the heuristics produce a definitive YES or NO.
When all the heuristics result in MAYBE then run one of the slower
exact algorithms that were discussed in the previous section.  Most of
the heuristics we considered were deterministic, but a few were
randomized.  This is again in the Las Vegas sense: An output of YES or
NO is always correct, but as function of the input and internal
algorthmic randomness MAYBE can be also be output and can vary between
independent runs.  The heuristics have several different forms, but
all rely ultimately on structural properties of strong USP.

\subsubsection{Downward Closed}

The simplest heuristics we considered were based on the fact that
strong USP are downward closed, that is, if $P$ is a strong USP, then
so is every subpuzzle $P' \sse P$.  This leads to practical heuristic
that can determine that a puzzle is not a strong USP.

\begin{algorithm}
  \caption{: Downward-closed Heuristic}
  \label{alg:downward-closed}
\begin{algorithmic}[1]
  \Require{An $(s,k)$-puzzle $P$, and size $s' \le s$.}
  \Ensure{NO, if $P$ has a set of $s'$ rows that do not form a strong USP, and MAYBE otherwise.}
  \Function{HeuristicDownwardClosed}{$P, s'$}
  \For{$P' \sse P, |P'| = s'$}
      \If{$P'$ is not a strong USP} \Return{NO.} \EndIf
  \EndFor{}
  \State{\Return{MAYBE}.}
  \EndFunction
\end{algorithmic}
\end{algorithm}

This algorithm runs in time $O(s^{s'} T_{Verify}(s', k))$ where
$T_{Verify}(s',k)$ is the running time for exactly verifying a
$(s',k)$-puzzle.  In practice we did not apply this heuristic for $s'$
larger than $3$, so the effective running time was $O(s^3 T(3,k))$,
which is polynomial in $s$ and $k$ using the verification algorithms
from the previous section which eliminate dependence on $k$ for
polynomial cost.  This heuristic can be made even more practical by
caching the results for puzzles of size $s'$, reducing the
verification time per iteration to constant in exchange for
$O(2^{s'k}T(s',k))$ time to precompute the values for all puzzles of
size $s'$.  There is also space overhead because the precomputed
results are being cached.  We also note that for a puzzle $P$ that is
a strong USP, the heuristic takes $\Theta(s^3 T(3,k))$ time without
cache or $\Theta(s^3)$ with caching.

Note that since our search algorithms typically start from a known
strong USP and tries to add rows, looking a subsets of a puzzle that
have already been verified is wasteful -- these are skipped over in
this setting.  From a practical point of view, running this heuristic
is free for small constant $s'$, as the exact verification algorithms
have a matching or higher polynomial running time.  Furthermore, since
the algorithm can return early, its expected running time on random
non-strong USPs is low.  There appeared to be dimenishing returns with
increasing $s'$ as it substantially increases precompute time and
storage while each stored value contributes relatively less to
verification as there are now many more values stored.

\subsubsection{Random}

XXX - There's also a random reordering heuristic, but I don't remember
the justification for it.  It's more elaborate than the other, but
less motivated.

\begin{algorithm}
  \caption{: Random Heuristic}
  \label{alg:random}
\begin{algorithmic}[1]
  \Require{An $(s,k)$-puzzle $P$, and iteration bound $t$.}
  \Ensure{NO, if a witness is found for $P$ not being a strong USP, and MAYBE otherwise.}
  \Function{HeuristicRandom}{$P$}
  \State{XXX - Fill out.}
  \EndFunction
\end{algorithmic}
\end{algorithm} 

\subsubsection{Greedy}

This heuristic attempts to solve the 3D-Matching instance associated
with verifying a puzzle $P$ (discussed in \autoref{sec:3DM}).  The
heuristic proceeds iteratively, determining the layer of the 3DM
instance with the least edges and randomly selecting an edge in that
layer to put into the 3DM.  If the heuristic successfully contructs a
3DM it returns NO indicating that the input puzzle $P$ is not a strong
USP.  If the heuristic reaches point were prior commits have made the
matching infeasible, the heuristic starts again from scratch.  This
process is repeated some number of times before it gives up and
returns MAYBE.  In our practical implementation we use $s^3$
iterations because the time balances well with the other heuristics
and effectively reduced the number of instances requiring a full
verification.

\begin{algorithm}
  \caption{: (Random) Greedy Heuristic}
  \label{alg:random-greedy}
\begin{algorithmic}[1]
  \Require{An $(s,k)$-puzzle $P$, and iteration bound $t$.}
  \Ensure{NO, if a witness is found for $P$ not being a strong USP, and MAYBE otherwise.}
  \Function{HeuristicGreedy}{$P$}
  \State{Construct 3DM instance $G : [s] \times [s] \times [s] \rightarrow \set{0,1}$ using $P$.}
  \For{$i = 1$ to $t$}
    \For{$j = 1$ to $s$}
      \State{$counts[j] =$ Number of edges incident vertex $j = \sum_{q,r} G(j,q,r)$.}
    \EndFor
    \State{Let $J,Q,R = \emptyset.$}
    \State{$m = 1.$}
    \While{$m \le s$}
      \State{\texttt{// Select edge to add.}}
      \State{Select $j \in \condset{j \in \bar{J}}{counts[j] = \max_{q \in \bar{J}} counts[q]}$ uniformly at random.}
      \If{$counts[j] \le 0$} break. \EndIf
      \State{Select $(q,r) \in \condset{(q,r) \in \bar{Q} \times \bar{R}}{G(j,q,r) = 1}$ uniformly at random.}
      \State{\texttt{// Update edges counts.}}
      \For{$u = 1$ to $s$}
        \For{$v = 1$ to $s$}
          \If{$(u,v) \in \bar{Q} \times \bar{R}$ and $G(j,u,v) = 1$} $counts[j] = counts[j] - 1$. \EndIf
          \If{$(u,v) \in \bar{J} \times \bar{R}$ and $G(u,q,v) = 1$ and $u \neq j$} $counts[u] = counts[u] - 1$. \EndIf
          \If{$(u,v) \in \bar{J} \times \bar{Q}$ and $G(u,v,r) = 1$ and $u \neq j$ and $v \neq q$} $counts[u] = counts[u] - 1$. \EndIf 
        \EndFor
      \EndFor
      \State{$J = J \cup \set{j}$.}
      \State{$Q = Q \cup \set{q}$.}
      \State{$R = R \cup \set{r}$.}
      \State{$m = m + 1.$}
      \EndWhile
    \If {$m > s$} \Return{NO}. \EndIf
  \EndFor
  \State{\Return{MAYBE}.}
  \EndFunction
\end{algorithmic}
\end{algorithm}

The array $counts$ is used to store the number of edges $counts[j]$
that remain associated with vertex $j$ along the first coordinate.
Much of the algorithm is devoted to maintaining this invariant.  The
arrays $J,Q,R$ store the vertices along the three coordinates,
respectively, that have already been incorporated into the partial 3D
matching.  Like in our previous algorithms we do not store the
matching itself, only the vertices involved.  The break at Line 10
triggers when the partial 3D matching is a dead end and cannot be
extended into a full 3D matching.  The condition of Line 22 is true
when a full 3D matching has been constructed and causes the algorithm
to return that $P$ is not a strong USP.

The running time of this algorithm is $O(s^3 t + T_{3DM}(s,k))$, where
$T_{3DM}(s,k)$ is the time required to construct 3D matching
instances from $(s,k)$-puzzles.  This algorithm has the potential to
be considerably slower than the downward-closure heuristic, especially
when $t = s^3$.  However, the main loop can terminate early at Line 10
when it fails to extend the 3D matching this permits the expected time
to much less than the worst case.  For a puzzle $P$ that is a strong
USP, the heuristic takes the full $\Omega(s^3 t + T_{3DM}(s,k))$ time.  


\begin{comment}

 
\subsubsection{Graph Automorphism}

A strong uniquely-solvable puzzle must also be a uniquely-solvable
puzzle.  Given a puzzle $P$ we can construct a graph $G_P$ such that
$G_P$ is rigid iff $P$ is a uniquely-solvable puzzle.

XXX - I don't think this idea worked out.  The code correctly
implemented the approach, but the approach was flawed.  Probably
remove this section or add to future work.

\end{comment}

\subsubsection{2D Matching}

The final heuristic we present is one-sided in the opposite direction
of the others, it may return YES or MAYBE.  In order for a hypergraph
$G = \langle [s] \times [s] \times [s], E\rangle$ to have a 3D
matching it must be the case that when any one of the three parts of
vertices of $G$ is projected away the resulting bipartite graph has a
perfect matching.  Thus we can witness that there is no 3D matching by
determining that one of three projected bipartite graphs $G|_1, G|_2,
G|_3$ does not have a perfect matching. 

\begin{algorithm}
  \caption{: 2D Matching Heuristic}
  \label{alg:2dm}
\begin{algorithmic}[1]
  \Require{An $(s,k)$-puzzle $P$.}
  \Ensure{YES, if $P$ is found to be strong USP, and MAYBE otherwise.}
  \Function{Heuristic2DMatching}{$P$}
  \State{Construct 3DM instance $G : [s] \times [s] \times [s] \rightarrow \set{0,1}$ using $P$.}
  \State{\texttt{// Construct projections.}}
  \State{Define projection $G|_1(b,c) = \vee_{a \in [s]} G(a,b,c)$.}
  \State{Define projection $G|_2(a,c) = \vee_{b \in [s]} G(a,b,c)$.}
  \State{Define projection $G|_3(a,b) = \vee_{c \in [s]} G(a,b,c)$.}

  \If{$G|_1, G|_2,$ and $G|_3$ have bipartite perfect matchings}
  \State{\Return{MAYBE.}}
  \Else
  \State{\Return{YES.}}
  \EndIf
  \EndFunction
\end{algorithmic}
\end{algorithm}

We can efficiently decide bipartite perfect matching using the
standard reduction to network flow and solve it using the
Ford-Faulkerson augmenting flow algorithm.  This approach yields an
algorithm that runs in time $O(s^3 + T_{3DM}(s,k))$, where
$T_{3DM}(s,k)$ is the time required to construct 3D matching instances
from $(s,k)$-puzzles.

In practice we found this heuristic not to be effective because the
projected $G|_i$ are typically dense graphs even if $G$ is not, and so
they are likely to have perfect matchings independently of whether $G$
has a 3D matching.



\subsection{Hybrid Algorithm}

Our final verification algorithm is a hybrid of several verification
algorithms and heuristics.  The size thresholds for which algorithm
and heuristic to apply were determined experimentally for small $k$
and were focused on the values were our strong USP search algorithms
were tractable $k \le 6$ (or nearly tractable $k \le 8$).  We decided
to run both the reduction to SAT and MIP in parallel because it was
not clear which algorithm performed better.  Since verification halts
when either algorithm completes the wasted effort is within a factor
of two of what the better algorithm could have done alone.  We also
chose to do this because we experimentally observed that there were
many instances that one of the algorithms struggled with that the
other did not.  We end up not using \textsc{Heuristic2DMatching}
because it rarely produced definitive output.

\begin{algorithm}
  \caption{: Final Hybrid Verification Algorithm}
  \label{alg:2dm}
\begin{algorithmic}[1]
  \Require{An $(s,k)$-puzzle $P$.}
  \Ensure{YES, if $P$ is found to be strong USP, and MAYBE otherwise.}
  \Function{Verify}{$P$}
  \If{$s \le 2$} \Return{\Call{VerifyBruteForce}{$P$}.} \EndIf
  \If{$s \le 7$} \Return{\Call{VerifyBidirectional}{$P$}.} \EndIf
  \If{$s \le 10$}
    \State{Return result if \Call{HeuristicDownwardClosed}{$P, 2$} is not MAYBE.}
    \State{\Return{\Call{VerifyBidirectional}{$P$}.}}
    \EndIf
  \State{Return result if \Call{HeuristicDownwardClosed}{$P, 3$} is not MAYBE.}
  \State{Return result if \Call{HeuristicRandom}{$P$} is not MAYBE.}
  \State{Return result if \Call{HeuristicGreedy}{$P$} is not MAYBE.}
  \State{Run \Call{VerifySAT}{$P$} and \Call{VerifyMIP}{$P$} in parallel and return the first result.}
  \EndFunction
\end{algorithmic}
\end{algorithm}

\section{Searching for Strong USPs}
\label{sec:search}

In certain respects, the problem of constructing a large strong USP is
much like the problem of constructing a large set of linearly
independent vectors in a vector space.  Indeed, the object to be
constructed is a set and the order which elements are added does not
matter, further the underlying elements are represented as a vector.
There are polynomial-time algorithms for determining whether a set of
vectors are independent, and we have a practical algorithm for
deciding whether a puzzle is a strong USP. 

There is a straightforward algorithm for constructing maxmimum size
sets of independent vectors: Start with an empty set $S$, and
repeatedly add vectors to $S$ which are linearly independent of the
vectors currently in $S$.  After this process completes, $S$ is a
largest set of linearly independent vectors.  This problem admits such
a greedy algorithm because the sets of linearly independent vectors
form a matroid.  The vector to be added each step can be computed
efficiently by solving a linear system of equations for vectors in the
null space of $S$.  Altogether this gives an efficient algorithm to
compute a maximum set of independent vectors or, more generally, to
complete a maximum set with some given subset $S'$.

Unfortunately this same approach does not work for generating maximum
size strong USPs.  The set of strong USP do not form a matroid, rather
they only form an independence system.  In particular, the empty set
is a strong USP and the set of strong USPs are downward closed,
however, the augmentation property required by matroids is not met.
Indeed, it does not hold even for 2-by-2 strong USPs (XXX - give more
specific example).  One consequence is that greedy algorithms are
unlikely to be effective for finding large strong USP.  Moreover, we
do not currently have a practical (or efficient) algorithm that can
take a strong USP $P$ and determine the set of rows that can extend
$P$ while preserving that it is a strong USP.

That said, we have had some success applying general purpose search
techniques together with our practical verification algorithm to
construct maximum size strong USPs for small $k$.  In particular, we
implemented variants of depth-first search (DFS) and breadth-first
search (BFS) in both desktop and HPC settings.



\subsection{Naive Search}


\subsection{Map-Reduce}

\begin{comment}
  Not planning to discuss:
  \begin{itemize}
  \item A$^*$ + admissible heuristics.
  \item Upper bounds from A$^*$.
  \item Symmetry removal.
  \end{itemize}
\end{comment}

\section{Experimental Results}
\label{sec:results}

\subsection{Strong USPs Found}
\label{subsec:usps_found}

\begin{figure}
  \label{fig:examples}
  \begin{multicols}{4}
  (1,1):\\[.5ex]
  \begin{tabular}{|c|}
    \hline
    1 \\ \hline
  \end{tabular}\\[6ex]

  (2,2):\\[.5ex]
  \begin{tabular}{|c|c|}
    \hline
    1&3 \\ \hline
    2&1 \\ \hline
  \end{tabular}\\[16ex]

  (3,3):\\[.5ex]
  \begin{tabular}{|c|c|c|}
    \hline
    1&1&1 \\ \hline
    3&2&1 \\ \hline
    3&3&2 \\ \hline
    \end{tabular}\\[2ex]

  (5,4):\\[.5ex]
  \begin{tabular}{|c|c|c|c|}
    \hline
    3&1&3&2 \\ \hline
    1&2&3&2 \\ \hline
    1&1&1&3 \\ \hline 
    3&2&1&3 \\ \hline 
    3&3&2&3 \\ \hline
  \end{tabular}\\[10ex]

  (8,5):\\[.5ex]
  \begin{tabular}{|c|c|c|c|c|}
    \hline
    3&3&3&1&1 \\ \hline
    1&1&2&2&1 \\ \hline
    2&1&3&3&2 \\ \hline
    3&2&2&2&3 \\ \hline
    2&1&2&1&3 \\ \hline
    2&2&3&1&2 \\ \hline
    3&2&3&2&1 \\ \hline
    3&1&2&1&1 \\ \hline
  \end{tabular}\\[16ex]
    
  (14,6):\\[.5ex]
  \begin{tabular}{|c|c|c|c|c|c|}
    \hline
    2&3&3&1&1&1 \\ \hline
    2&1&1&2&1&1 \\ \hline
    3&3&1&2&1&1 \\\hline
    3&2&2&2&1&1 \\\hline
    2&3&1&1&2&1 \\\hline
    2&2&3&1&2&1 \\\hline
    3&3&1&3&2&1 \\\hline
    3&2&3&3&2&1 \\\hline
    2&1&1&3&1&2 \\\hline
    2&3&1&3&2&2 \\\hline
    3&1&1&1&1&3 \\\hline
    3&3&2&3&1&3 \\\hline
    3&3&2&1&2&3 \\\hline
    2&2&3&2&2&3 \\\hline
  \end{tabular}
  \end{multicols}
  \caption{Respresentative examples of the largest strong uniquely
    solvable $(s,k)$-puzzles found from width $k = 1$ to $6$.}
\end{figure}

\autoref{table:compare} summarizes our main results which are
improved bounds on the maximum size of strong uniquely solvable
puzzles and compares them to bounds from \cite{cksu05}.  XXX
\begin{table}
  \label{table:compare}
  \begin{center}
  \begin{tabular}{|c|r|r|r|r|}
    \hline
    & \multicolumn{2}{|c|}{[CKSU05]} & \multicolumn{2}{|c|}{This work} \\
    \hline
    $k$ & Maximum $s$ & $\omega^*$~ & Maximum $s$ & $\omega^*$~\\
    \hline
    1 & $s\le ~~~~1$ & & $1=s$ & $3.000$  \\
    2 & $s\le ~~~~3$ & & $2=s$ & $2.670$ \\
    3 & $3 \le s \le ~~~~6$ & $2.642$ & $3=s$ & $2.642$ \\
    4 & $s\le ~~12$ & & $5=s$ & $2.585$ \\
    5 & $s\le ~~24$ & & $8=s$ & $2.562$  \\
    6 & $10 \le s \le ~~45$ & $2.615$ &$14\le s$ & $2.521$\\
    7 & $s\le ~~86$ & & $21\le s$ & $2.531$ \\
    8 & $s\le 162$ & & $30\le s$ & $2.547$ \\
    9 & $36 \le s \le 307$ & $2.592$ &$42\le s$ & $2.563$  \\
    \hline
  \end{tabular}
  \end{center}
  \caption{Comparison of bounds on maximum size of strong uniquely
    solvable $(s,k)$-puzzles with \cite{cksu05}.  $\omega^*$ is the
    $\omega$ in the limit of composing puzzles of these dimensions via
    direct product.}
\end{table}
  
\subsection{Algorithm Performance}
\label{subsec:performance}

\section{Conclusions}
\label{sec:conclusion}



\bibliographystyle{customurlbst/alphaurlpp} \bibliography{references}

\appendix


\end{document}
