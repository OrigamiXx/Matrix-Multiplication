\documentclass[11pt]{article}
\usepackage{fullpage}
\usepackage{rpmacros}
\RequirePackage[colorlinks=true]{hyperref}
\hypersetup{
  linkcolor=[rgb]{0,0,0.4},
  citecolor=[rgb]{0, 0.4, 0},
  urlcolor=[rgb]{0.6, 0, 0}
}
\usepackage{mathpazo}
\usepackage{bbm}
\usepackage{todonotes}
\usepackage{lipsum}
\usepackage{setspace}
\usepackage{mdframed}
\usepackage{multicol}
\usepackage{graphicx}
\usepackage{comment}
\usepackage{tikz}
\usetikzlibrary{decorations.pathreplacing,backgrounds}

\usepackage[font=footnotesize]{caption}

\input{thmmacros}
\input{customurlbst/bibmacros}

\usepackage{algorithmicx}
\usepackage{algorithm} % http://ctan.org/pkg/algorithms
\usepackage[noend]{algpseudocode}
\renewcommand{\algorithmicensure}{\textbf{Output:}}
\renewcommand{\algorithmicrequire}{\textbf{Input:}}


\algrenewcommand\algorithmicindent{1.0em}%

\newcommand{\RPnote}[1]{\textcolor{BrickRed}{RP: #1}}
\newcommand{\Mattnote}[1]{\textcolor{OliveGreen}{MWA: #1}}
\newcommand{\BLnote}[1]{\textcolor{Blue}{BLV: #1}}
\newcommand{\Anote}[1]{\textcolor{Plum}{A: #1}}
\newcommand{\MFnote}[1]{\textcolor{DarkOrchid}{MF: #1}}

\newcommand*\samethanks[1][\value{footnote}]{\footnotemark[#1]}
\newcommand\sse{\subseteq}
\newcommand\Sym[1]{\ensuremath{\mathrm{Sym}_{#1}}}
\newcommand\condset[2]{\set{#1 \;|\; #2}}
\renewcommand\NP{\ensuremath{\mathsf{NP}}}
\newcommand\coNP{\ensuremath{\mathsf{coNP}}}

%\onehalfspacing
\date{}

\title{Matrix Multiplication: Finding Strong Uniquely-Solvable Puzzles
{\IfFileExists{./sha.tex}{\\\small SHA: \input{sha}}{}}}
\author{
Matthew Anderson\thanks{Department of Computer Science, Union College, Schenectady, New York, USA, E-mails: \texttt{andersm2@union.edu, jiz@union.edu, xua@union.edu}}%
\and%
Zongliang Ji\samethanks[1]
\and%
Anthony Yang Xu\samethanks[1]
}
\begin{document}
\maketitle

\begin{abstract}

\end{abstract}

\thispagestyle{empty}
\newpage
\pagenumbering{arabic}


\section{Introduction}
\label{sec:intro}

% Context

An optimal algorithm for matrix multiplication is remains elusive
despite substantial effort.  We focus on the square variant of the
matrix multiplication problem, that is, given two $n$-by-$n$ matrices
$A$ and $B$ over a field $\F$, the goal is to compute the matrix
product $C = A \times B$.  The question that arised is: How many field
operations are required to compute $C$.  The na\"{i}ve algorithm,
based on the mathematical definition of matrix product, runs in time
$O(n^3)$, and for a time it was thought to be the optimal algorithm.
It was surprising when Strassen showed that matrix multiplication can
be done in time $O(n^{2.808})$ \cite{str69}, using a
divide-and-conquer approach.  A long sequence of work concluding with
Coppersmith and Winograd's laser method that reduced the running time
to $O(2^{2.376})$ \cite{pan78,b79,sch81,cw82,str86,cw87}. Recent
computer-aided refinements of Coppersmith and Winograd's work done by
Stothers, Vassilevska-Williams, and Le Gall further reduced the
exponent to $\omega \le 2.3728639$ \cite{sto10,vas11,leg14}.

There are some lower bounds on the time complexity of matrix
multiplication.  Na\"{i}ely, the dimensions of the output matrix $C$
imply that the problem requires at least $\Omega(n^2)$ time.  Slightly
better lower bounds are also known for specialized models of
computation, like bounded-depth arithmetic circuits \cite{XXX}.  There
are also lower bounds known for a variety of algorithmic approaches
for matrix multiplication.  Ambainis et al showed that the laser
method, the approach of Coppersmith-Winograd and subsequent
refinements, cannot alone achieve an algorithm with $\omega \le
2.3078$ running time \cite{afl14}.

% Other lower bounds?
% Tensor rank approach
% - Lower bounds
% - Impossibility

% Motivation

Cohn and Umans \cite{cu03} introduced a program for developing faster
algorithms for matrix multiplication by reducing this question to a
search for groups with subsets that satisfy a certain algebraic
property called the \emph{triple-product property} that allows matrix
multiplication to be embed in the group algebra.  Subsequent work
\cite{cksu05} elaborated on this idea and developed the notion of
\emph{strong uniquely solvable puzzles} (SUSP) whose existence would
also imply faster matrix multiplication algorithms.

A \emph{width}-$k$ puzzle $P$ is a subset of $U_k = \set{0,1,2}^k$,
and the cardinality of $P$ is the puzzle's \emph{size}.  Each element
of $P$ is called a \emph{row} of $P$, and each row consists of three
\emph{subrows} which are elements of $\set{0,*}^k$, $\set{1,*}^k$,
$\set{2,*}^k$ respectively.  Informally, a puzzle $P$ is a uniquely
solvable puzzle if there is no way to permute the subrows of $P$
without overlap to form a distinct puzzle $P'$.  A uniquely solvable
puzzle is \emph{strong} if stronger condition for non-overlapping is
applied. For a fixed width $k$, the larger the size of a puzzle $P$,
the faster matrix multiplication algorithm it implies if $P$ is a SUSP
\cite[Corollary 3.6]{cksu05}.  In fact Cohn et al.~show that there
exist an infinite family of SUSP that achieve $w < 2.48$
\cite[Proposition 3.8]{cksu05}.

% Approach

We follow Cohn and Umans' program simulateneously from theoretical and
experimental perspectives.  We explore properties of SUSP, develop
\emph{verification} algorithms for determining whether a puzzle is a
SUSP, develop \emph{search} algorithms for searching for large SUSP,
and implement these algorithms in desktop and high performance
computing settings.  From the computational complexity perspective the
algorithms we develop to verify and search for SUSP are not efficient
as they run in exponential or doubly exponential time in the natural
parameters.  However, as the goal is to find a sufficiently large SUSP
which in turn produces a fast matrix multiplication algorithm, the
inefficiency of our algorithms does not directly impact the efficiency
of the matrix multiplication algorithms.  Rather, it indirectly
impacts the efficiency by limiting the search space of puzzles that we
can practically examine.

% Results

In addition to the various theoretical results, we have experimental
results that bound the size of the largest SUSP for small width
puzzles.  For small constant width the bounds are tight, though the
tightness of the results decreases as width increases.  Lower bounds
on size are witnessed by examples of SUSP we found via search and
constructions that compose SUSP of small width into SUSP of larger
width while maintaining the relative size of the SUSP.  Upper bounds
are determined computationally by evaluating admissible heuristics on
the initial levels of the puzzle search tree.  Although our current
experimental results do not beat Proposition 3.8 of \cite{cksu05} for
unbounded $k$, they do suggest there is considerable room for
improvement.

% Organization
\paragraph{Organization}
We begin with formal definitions and properties of puzzles and strong
uniquely solvable puzzles in \autoref{sec:prelim}.  In
\autoref{sec:verify} we discuss our algorithms for verifying that a
puzzle is a SUSP.  In \autoref{sec:heuristic} we describe fast,
but incomplete, heuristics for verifying SUSP.
\autoref{sec:search} explains our search algorithms.
\autoref{sec:results} discuss our implementation, experiments, and
the experimental results.


\section{Preliminaries}
\label{sec:prelim}

\newcommand\ordset[1]{\ensuremath{[#1]}}

We use $\ordset{n}$ to denote the set $\set{0,1,2,\ldots, n-1}$.  For
a set $Q$, $\Sym{Q}$ denotes the symmetric group on the elements of
$Q$.  \cite{cksu05} introduced the idea of a \emph{puzzle}.

\begin{definition}[Puzzle]
  For $s, k \in \Natural$, an $(s,k)$-\emph{puzzle} is a
  subset $P \sse U_k = \ordset{3}^k$ with $|P| = s$.
\end{definition}

We say that an $(s,k)$-puzzle has $s$ rows and $k$ columns.  The
columns are inherent ordered and indexed by $\ordset{k}$.  The rows are not
inherently ordered, however, it is often convienent to assume that the
rows are arbitrarily ordered and indexed by $\ordset{s}$.

\cite{cksu05} also establish a particular combinatorial property of
such puzzles that can derive groups that matrix multiplication can be
embedded into.  Such puzzles are called \emph{strong} uniquely
solvable puzzles.  However, to give some intuition we first explain a
simpler version of the property called \emph{uniquely solvable
  puzzles}.

\begin{definition}[Uniquely Solvable Puzzle (USP)]
  \label{def:strong-USP}
  ~\\An $(s,k)$-puzzle $P$ is \emph{uniquely solvable} if
  $\forall \pi_0, \pi_1, \pi_2 \in \Sym{P}:$
  \begin{enumerate}
  \item either $\pi_0 = \pi_1 = \pi_2$, or
  \item $\exists r \in P, \exists i \in \ordset{k}$ such that at least two
    of the following hold:
    \begin{enumerate}
    \item $(\pi_0(r))_i = 0$,
    \item $(\pi_1(r))_i = 1$,
    \item $(\pi_2(r))_i = 2$.
    \end{enumerate}
  \end{enumerate}
\end{definition}

Informally a puzzle is \textbf{not} uniquely solvable if each row of
the puzzle can be broken into zeros, ones, and twos pieces and then
the rows can be reassembled in a different way so that each new row is
a combination of a zeroes, a ones, and twos piece where there is
exactly element of $\ordset{3}$ for each column.  Observe that uniquely
solvable puzzles can have at most $2^k$ rows because each zeroes
piece, ones piece, and two piece must be unique, as otherwise the
duplicate pieces can be swapped making the puzzle not uniquely
solvable.  The definition of \emph{strong} uniquely solvable puzzle is
below, it is nearly the same except that it requires that there be a
collision on a column between exactly two pieces, not two or more
pieces like in the original definition.

\begin{definition}[Strong Uniquely Solvable Puzzle]
  ~\\
  An $(s,k)$-puzzle $P$ is \emph{strong uniquely solvable} if
  $\forall \pi_0, \pi_1, \pi_2 \in \Sym{P}:$
  \begin{enumerate}
  \item either $\pi_0 = \pi_1 = \pi_2$, or
  \item $\exists r \in P, \exists i \in \ordset{k}$ such that exactly two
    of the following hold:
    \begin{enumerate}
    \item $(\pi_0(r))_i = 0$,
    \item $(\pi_1(r))_i = 1$,
    \item $(\pi_2(r))_i = 2$.
    \end{enumerate}
  \end{enumerate}

\end{definition}

Observe that the properties of uniquely solvable and strong uniquely
solvable are invariant to the ordering of the rows or columns of a
puzzle.  This fact is used implicitly.  Also note that these
properties are invariant to maps between puzzles induced by
permutations from \Sym{\ordset{3}} on the puzzle cell elements.

Cohn et al.~ show the following connection between the existence of
strong uniquely solvable puzzles and upper bounds on the exponent of
matrix multiplication $\omega$.

\begin{lemma}[{\cite[Corollary 3.6]{cksu05}}]
  If there is a strong uniquely solvable $(s,k)$-puzzle,
  $$\omega \le \min_{m \ge 3, m \in \Natural} \frac{3 \log
    m}{\log(m-1)} - \frac{3 \log s!}{sk \log(m-1)}.$$
\end{lemma}

In the same article, the authors also demonstrate the existence of an
infinite family of strong uniquely solvable puzzles that achieves a
non-trivial bound on $\omega$.

\begin{lemma}[{\cite[Proposition 3.8]{cksu05}}]
  There is an infinite family of strong uniquely solvable puzzles that
  achieves $\omega < 2.48$.
\end{lemma}

Finally, they conjecture that strong uniquely solvable puzzles provide
a root to achieving quadratic time matrix multiplication.

\begin{conjecture}[{\cite{cksu05}}]
  There exists a family of strong uniquely solvable puzzles that
  implies $\omega = 2$.
\end{conjecture}

Unfortunately, this conjecture was recently shown to be false.

\begin{lemma}[\cite{bccgu16}]
  Strong uniquely solvable puzzles cannot show $\omega < 2 +
  \epsilon$, for some $\epsilon > 0$.
\end{lemma}

This result is a consequence of a recent breakthrough arithmetic
progressions in cap sets \cite{e16,clp16} combined with a conditional
result on the Erd\"{o}s-Szemeredi sunflower conjecture \cite{asu13}.
The results of \cite{bccgu16} do imply that Cohn and Umans' strong
uniquely solvable puzzle approach cannot achieve the ideal $\omega =
2$.  However, we are unaware of a concrete lower bound on $\epsilon$
implies by this result.  This means there is a still a substantial gap
in our understanding between what has been acheived by the refinements
of LeGall, Williams, and Stothers, and the impossibility of showing
$\omega = 2$ using the Cohn and Umans' approach.



\section{Verifying Strong USPs}
\label{sec:verify}

The core focus of this article is the verification of strong
uniquely-solvable puzzles.  In particular, we are interested in the
decision problem: Given an $(s,k)$-puzzle $P$, output YES iff $P$ is a
strong uniquely-solvable puzzle.  In this section we discuss the
design of efficient and practical algorithms to solve this problem as
a function of the parameters $s$ and $k$.  As the goal of this work is
to use computers to locate large strong USPs, we also discuss aspects
our implementation that informed or constrained our designs.  All of
the exact algorithms we develop in this section have exponential
running time in the parameters $s$ and $k$.  However, although we will
discuss the asymptotic worst-case running time of our algorithms, this
is not the metric we are truly interested in.  Rather we are
interested in the practical performance of our algorithms and their
capability for locating new large strong USPs.

The algorithm that we ultimately develop and implement in this section
is a hybrid of a number of simpler algorithms and heuristics.  The
primary reason for this is that the algorithms with better asymptotic
and practical performance at larger input lengths have large overhead
at small input lengths.  Although we have two parameters $s$ and $k$
they are not fully independent.  First, $s \le 3^k$ because the
maximum number of rows in a puzzle of width $k$ is $|[3]^k| = 3^k$.
Second, we can eliminate the dependence on $k$ entirely by
transforming an $(s,k)$-puzzle into a 3D matching instance on the
vertex set $[s]^3$.  However, this transformation is not free because
the instance size is now a function of the cube of $s$ rather than
linear in $s$.

\subsection{Brute Force}

The obvious algorithm for verification comes directly from the
definition of strong uniquely solvable puzzles
(\autoref{def:strong-USP}).

\begin{algorithm}
  \caption{: Brute Force}
  \label{alg:brute-force}
\begin{algorithmic}[1]
  \Require{An $(s,k)$-puzzle $P$.}
  \Ensure{YES, if $P$ is a strong USP and NO otherwise.}
  \Function{VerifyBruteForce}{$P$}
  \For{$\pi_1 \in \Sym{P}$}
    \For{$\pi_2 \in \Sym{P}$}
      \If{$\pi_1 \neq 1 \vee \pi_2 \neq 1$}
        \State{$found = false.$}
        \For{$r \in P$}
          \For{$i \in [k]$}
            \If{$\delta_{r_i, 0} + \delta_{(\pi_1(r))_i, 1} + \delta_{(\pi_2(r))_i, 2} = 2$} $found = true$. \EndIf
          \EndFor
        \EndFor
        \If{not $found$} \Return{NO.} \EndIf
      \EndIf
    \EndFor
  \EndFor
  \State{\Return{YES}.}
  \EndFunction
\end{algorithmic}
\end{algorithm}
Note that $1$ in Line 3 denotes the identity permutation in $\Sym{P}$,
and $\delta_{a,b}$ is the Kronecker delta function which is $1$ if $a
= b$ and $0$ otherwise.  Observe that \autoref{alg:brute-force} does
not refer to $\pi_0$ of \autoref{def:strong-USP}.  This is because the
strong USP property is invariant to permutations of the rows and so
$\pi_0$ can be thought of as an arbitrary phase.  We fix $\pi_0 = 1$
to simplify the algorithm.  Noting that $|\Sym{P}| = s!$, the four
nested loops make the running time of this algorithm is easy to
analyze.  The algorithm runs in time $O((s!)^2 \cdot s \cdot k \cdot
\poly(s))$ where the last factor low polynomial factor of $s$ accounts
for the time to perform operations on permutations.  The dominant term
in the running time is the contribution from iterating over pairs of
permutations, though this term was made a factor $s!$ smaller by not
iterating over $\pi_0$.  Finally, notice that if $P$ is a strong USP,
then the algorithm runs in time $\Theta((s!)^2 \cdot k \cdot
\poly(s))$, and that if $P$ is not a strong USP the algorithm may
terminate early.  This algorithm's poor theoretical and practical
performance made it unusable in our implementation, however, it's
simplicity and direct connection to the definition made its
implementation a valuable sanity check against later more elaborate
algorithms (and served as effective onboarding to undergraduate
students helping with this project).

Although \autoref{alg:brute-force} performs poorly, examining the
structure of a seemingly trivial optimization leads to substantially
more effective algorithms.

Consider the following function on triples of rows $a, b,
c$ $$f(a,b,c) = \vee_{i \in [k]} (\delta_{a_i,0} + \delta_{b_i,1} +
\delta_{c_i,2} = 2).$$ We can replace the innermost loop in Lines 6 \&
7 of \autoref{alg:brute-force} with the statement $found = found \vee
f(r, \pi_1(r), \pi_2(r))$.  Observe that $f$ neither depends on $P$,
$r$, nor the permutations, and that \autoref{alg:brute-force} no
longer depends on $k$.  To slightly speed up \autoref{alg:brute-force}
we can precompute and cache $f$ before the algorithm starts and then
look up values as the algorithm runs.  There are two obvious options
to consider we can either precompute $f$ specialized to the rows in
the puzzle $P$, or we can precompute $f$ for all possible rows.  In
former case the time to precompute $f$ is $\Theta(s^3 \cdot k)$ and in
the later case $\Theta(3^{3k} \cdot k)$.  The storage requirements are
$\Theta(s^3)$ and $\Theta(3^{3k})$ bits respectively.  The former is
problematic for large $s$ and later problematic even for small $k$.
Moreover the combined running time for the two options with a single
call to verify a puzzle is $\Theta(s^3 \cdot k + (s!)^2 \cdot
\poly(s))$ and $\Theta(3^{3k} \cdot k + (s!)^2 \cdot \poly(s))$.  In
the former case there is an asymptotic improvement, but in the later
case, the saving of a factor of $k$ is easily offset by the additional
$3^{3k}$ term.  For this reason we rule out the later option and chose
to represent the function $f$ specialized to $f_P$ for a given puzzle
$P$.

\subsection{Verification to 3D Matching}

For verification it turns out to be more useful to work with $f_p$
than with $P$.  It is convienent to think of $f_P$ from several
perspectives: (i) as a function $f_P : P \times P \times P \rightarrow
\set{0, 1}$, (ii) as an order three tensor $f_p \in \set{0,1}^{P
  \times P \times P}$, and (iii) as the complement of the
characteristic function of hyperedge relations of a tripartite
hypergraph $H_P = \langle P \sqcup P \sqcup P, \bar{f_p}\rangle$ where
the vertex set is the disjoint union of three copies of $P$.  The last
of these, $H_P$, is the most useful.

Let $G = \langle P \sqcup P \sqcup P, E \sse P^3\rangle$ be a
tripartite 3-hypergraph.  We say that $G$ has a \emph{3D matching} iff
there exists a subset $M \sse E$ with $|M| = |P|$ and for all distinct
hyperedges $e_1, e_2 \in M$, $e_1$ and $e_2$ are \emph{vertex
  disjoint}, that is, $(e_1)_i \neq (e_2)_i$ for all $i \in [3]$.  We
say a 3D matching is \emph{nontrival} if it is not the matching
$\condset{(r,r,r)}{r \in P}$.

\begin{lemma}
  \label{lem:verify-to-3dm}
  An $(s,k)$-puzzle $P$ is a strong USP iff $H_P$ has no nontrivial 3D
  matching.
\end{lemma}

\begin{proof}
  We first argue the reverse direction.  Suppose that $H_p$ has a
  nontrivial 3D matching $M$.  We show that $P$ is not a strong USP by
  using $M$ to construct $\pi_0, \pi_1, \pi_2 \in \Sym{P}$ that
  witness this.  Let $\pi_0$ be the identity permutation.  For each $r
  \in P$, define $\pi_1(r) = q$ where $(r,q,\_) \in M$.  Note that $q$
  is well defined and unique because $M$ is 3D matching and has vertex
  disjoint edges.  Similarly define $\pi_2(r) = q$ where $(r,\_,q) \in
  M$.  Observe that by construction $M =
  \condset{(\pi_0(r),\pi_1(r),\pi_2(r))}{r \in P}$.  Since $M$ is a
  matching of $H_P$, $M \sse \bar{f_P}$.  Because $M$ is a nontrival
  matching at least one edge in $(a,b,c) \in M$ is has either $a \neq
  b$, $a \neq c$, or $b \neq c$.  This implies, respectively, that as
  constructed $\pi_0 \neq \pi_1$, $\pi_0 \neq \pi_2$, or $\pi_1 \neq
  \pi_2$.  In each case we have determined that $\pi_0$, $\pi_1$, and
  $\pi_2$ are not all identical.  Thus we determined permutations such
  that for all $r \in P$, $f(\pi_0(r), \pi_1(r), \pi_2(r)) = 0$.  This
  violates Condition 2 of \autoref{def:strong-USP}, hence $P$ is not a
  strong USP.

  The forward direction of the lemma is argued symmetrically.  Suppose
  that $P$ is not a strong USP. We show that $H_P$ has a 3D matching.
  For $P$ not to be a strong USP there must exist $\pi_0, \pi_1, \pi_2
  \in \Sym{P}$ not all identical such that Condition 2 of
  \autoref{def:strong-USP} fails.  Define $e(r) =
  (\pi_0(r),\pi_1(r),\pi_2(r))$ and $M = \condset{e(r)}{r \in P}$.
  Since Condition 2 fails, we have that $f_P(e(r)) = false$ for all $r
  \in P$.  This means that for all $r \in P$< $e(r) \in \bar{f_P}$ and
  hence $M \sse \bar{f_P}$.  Since $\pi_0$ is a permutation $|M| =
  |P|$.  Finally, observe that $M$ is nontrivial because not all of
  the permutations are identical and there must be some $r \in P$ with
  $e(r)$ having non identical coordinates.  Thus $M$ is a nontrivial
  3D matching of $H_p$.
\end{proof}

Although 3D matching is an \NP-complete problem \cite{karp72},
\autoref{lem:verify-to-3dm} does not immediately imply that
verification of strong USP is \coNP-complete because $H_P$ is not an
arbitrary hypergraph.  (That verification is in \coNP is immediate
from \autoref{def:strong-USP}.)  Indeed, it seems difficult to define a
puzzle $P$ that allows hyperedges in $H_P$ to be included
independently.  It remains open whether verification is
\coNP-complete, but our implementation and experimental data suggests
that it is likely not to be the case.

\autoref{lem:verify-to-3dm} implies that to verify $P$ is a strong USP
it suffices to determine whether $H_P$ has a 3D matching.  In the
subsequent sections we examine algorithms for the later problem.  We
can also view \autoref{alg:brute-force} as algorithm for solving 3D
matching.  Since we believe that verification is not \coNP-complete we
will use properties of strong USP and puzzles to provide additional
structure that algorithms can take advantage of.

XXX - Discuss simplifyTDM.

\subsection{Bidirectional Search}

The realization that verification of strong USP is a variant of 3D
matching quickly led to a linear exponential time algorithm which much
more practical.  The reduction allows us to replace the permutations
from $\Sym{P}$ with subsets of $P$ and effectively reduce the cost of
the outer loops of \autoref{alg:brute-force} from $s! =
\Theta(2^{s\log s}$ to $2^s$.  We now describe a recursive
bidirectional strong USP verification algorithm that uses this
observation.

\begin{algorithm}
  \caption{: Bidirectional}
  \label{alg:bi}
\begin{algorithmic}[1]
  \Require{An $(s,k)$-puzzle $P$.}
  \Ensure{YES, if $P$ is a strong USP and NO otherwise.}
  \Function{VerifyBidirectional}{$P$}
  \State{Let $T = \emptyset$.}
  \State{Construct $H_P$.}
  \Function{SearchHalf}{$\ell, Q,\ell_Q, R,\ell_R, \delta, t$}
  \If{$\ell = t$}
    \If{$\delta = 1$} \Comment{Forward Base Case}
      \State{Insert $(Q,R)$ into $T$.}
      \State{\Return{$false$}.}
    \Else \Comment{Reverse Base Case}
      \If{$(P-Q, P-R) \in T$} \State{\Return{$true$}.} \Else \State{\Return{$false$}.} \EndIf
    \EndIf
  \EndIf
  \State{$result = false$.} \Comment{Recursive Case}
  \For{$\ell'_Q = \ell_Q + 1$ to $s$}
    \For{$\ell'_R = \ell_R + 1$ to $s$}
      \If{$(p_\ell, p_{\ell'_Q}, p_{\ell'_R}) \in H_P$}
        \State{$result = result~\vee$ \textsc{SearchHalf}$(\ell + \delta, Q \cup \set{p_{\ell'_Q}}, \ell'_Q, R \cup \set{p_{\ell'_R}}, \ell'_R, \delta, t)$.}
      \EndIf
    \EndFor
  \EndFor
  \State{\Return{$result$.}}
  \EndFunction

  \State{\textsc{SearchHalf}$(1,\emptyset, 0, \emptyset, 0, 1, \lfloor s / 2 \rfloor + 1)$.}
  \State{\Return{\textsc{SearchHalf}$(s, \emptyset, 0, \emptyset, 0, -1, \lfloor s/2 \rfloor)$}.}
  \EndFunction
\end{algorithmic}
\end{algorithm}

\autoref{alg:bi} consists of two phases. Let $t = s/2 \rfloor$. he
first phase determines all possible sets $Q,R \sse P$ with $|Q| = |R|
= t$ such that there is 3D matching $M_1$ of $H_P$ when restricted to
the vertices $\set{p_1,p_2, \ldots, p_t} \sqcup Q \sqcup R$.  The sets
$Q,R$ satisfying the requirement are stored in $T$ during the first
phase on Line 6.  The second phase determines all possible sets $Q,R
\sse P$ with $|Q| = |R| = s - t$ such that there is a 3D matching
$M_2$ of $H_P$ when restricted to the vertices
$\set{p_{t+1},p_{t+2},\ldots,p_s} \sqcup Q \sqcup R$.  For each pair
$(Q,R)$ the algorithm determines in the second phase it checks whether
$(P - Q, P - R)$ was inserted into $T$ during the first phase.  If the
pair is present it means that there is a 3D matching of $H_P$ which is
$M = M_1 \cup M_2$.  This works because $M_1$ and $M_2$ are partial 3D
matchings and vertex disjoint.  This is because $M_1$ is a 3D matching
on $\set{p_1,\ldots,p_t}$ and $M_2$ is a 3D matching on
$\set{p_{t+1},\ldots p_s}$, and because in Line 9 of \autoref{alg:bi}
the lookup is performed on the complementary sets.  The second phase
returns whether a complete matching could be found, and hence by
\autoref{lem:verify-to-3dm} whether $P$ is a strong USP.  (Note that
the first phase always returns $false$.)

The running time of this algorithm is dominanted by the number of
pairs of sets $(Q,R)$ it examines.  Observe that rows of $P$ are
considered in order in Lines 14 \& 15.  Further, the algorithm tracks
the index of last elements added to $Q$ and $R$ in $\ell_Q$ and
$\ell_R$ respectively.  The algorithm only adds new elements to $Q$ or
$R$ that have higher indexes than ones previously added.  Altogether
this implies that each pair of sets $(Q,R)$ is only considered at most
once during a phase.  Since $Q, R \sse P$, there are at most $2^s
\cdot 2^s$ pairs $(Q,R)$.  This means that \textsc{SearchHalf} is
called at most $4^s$ times during each phase.  Hence the running time
of the algorithm is $O(4^s \cdot s^2 \cdot \poly(s) + T_{3DM}(s,k))$
where $s^2$ factor comes from the inner loops and $T_{3DM}(s,k)$
accounts for the time to construct $H_P$.  The memory requirements of
\autoref{alg:bi} are similarly high -- the first phase uses $O(4^s
\cdot s)$ to store $T$.

Note that algorithm does not early terminate on $P$ that are strong
USP, because it must search through all pairs before determining that
none can be found.  The algorithm could be modified to allow early
termination when $P$ is not a strong USP by causing the second phase
of search to immediately return in Line 17 once the first 3D matching
witness has been located.  However, this still requires the first
phase to run to completion.  A remedy for this would be to run both
phases in parallel and have them check against each other.  This would
substantially complicate the implementation, and not substantially
improve the practical running time for puzzles that are strong USP.

In the implementation of \autoref{alg:bi} we represented the sets
$Q,R$ using bit sets encoded into a single 64-bit long.  This
permitted it to represent these sets for $s \le 32$.  We implemented
$T$ using a hash table (maps from the C++ standard template library).
These choices made the basic data structure operations in
implementation effectively constant time, and have low memory overhead
beyond the contents of $T$.

\newpage
\subsection{3DM to 3SAT}

Following from \autoref{lem:verify-to-3dm}, we know that we are able to verify whether a puzzle $P$ is a strong USP or not by contructing a graph $H_P$ from $P$ and determine whether it has a 3D matching. We also know that 3D matching is an NP-complete problem which could be reduced to another NP-complete problem. Here, we first reduced our 3D matching problem to another NP-complete problem called the 3-satisfability (3SAT) problem and hope to utilize the state-of-art SAT solver to yield an answer in a more efficient way. To perform the reduction, we want to convert the graph $H_P$ that obtained from the puzzle $P$ into a desired conjunctive normal form (CNF) which are boolean variables connected by ANDs of ORs. Then we feed the CNF formula into a SAT solver $S$ and determine whether the formula is satisfiable. We know from \autoref{lem:verify-to-3dm} that the formula is satisfiable iff $H_P$ has a non-trivial \emph{3D matching}. 

We first convert each vertex on $H_P$ into cooresponding index of boolean variable. Normally, we would use three sets of indices $X, Y, Z = [s]$ to represent vertices from each copy of $P$ where $|X| = |Y| = |Z| = s$. Then the edge $E$ of $H_P$ would be represented by a set of boolean variables $Var$ indexed by these three sets of indicies, as $e \in E$ is equivalent to $Var_{x,y,z}$ where $x \in X, y \in Y, z \in Z$. By doing this, we have $s^3$ boolean variables in our formula. However, since the set $X, Y, Z$ has identical indicies, we are able to represent these $s^3$ edges by only $2s^2$ boolean variables by having two indexing sets $X', Y'$ each has $s^2$ elements. The $X'$ could be regarded as the combinations of subhyperedges from first copy of $P$ to the second copy of $P$. Similarly, the $Y'$ could be regarded as the combinations of subhyperedges from first copy of $P$ to the third copy of $P$.  The intuition comes from describe each point on a cube with two coordinates instead of three. (JJ: ``I can't elaborate more on this since I don't know how to tell the story on how we reduced the number of variables.'').

After converting the graph $H_P$ into these $2s^2$ variables, we will begin adding the clauses which each clause consists of boolean variable connected by only ORs. The clauses that we are adding correspond to three parts of checking whether the Graph has a non-trivial \emph{3D matching}. This generating and adding clauses procedure is described in \autoref{alg:cnf}. In this algorithm, we use $O(s^2)$ variables to generate $O(s^3)$ clauses for the solver by checking the uniqueness, existence and non-triviality of the 3D matching problem.

As for \autoref{alg:sat}, it mainly uses the \autoref{alg:cnf} to feed the CNF formula in and apply the Solver. The running time of this algorithm mainly determines by the sovling time of the Solver which we cannot estimate from a theoretical level. In the later section, we will show the performance of the MapleSAT solver on different cases of puzzles.


\label{subsec:sat}

\begin{algorithm}
  \caption{: Reduction to satisfiability}
  \label{alg:sat}
\begin{algorithmic}[1]
  \Require{An $(s,k)$-puzzle $P$.}
  \Ensure{YES, if $P$ is a strong USP and NO otherwise.}
  \Function{VerifySAT}{$P$}
  \State{Construct a empty Solver $S$}
  \State{Let S be \textsc{ConstructReduction}($P$, $S$)}
  \If{S is not solvable}
  \State{\Return YES}
  \Else
  \State{\Return NO}
  \EndIf
  \EndFunction
\end{algorithmic}
\end{algorithm}

\begin{algorithm}
  \caption{: Create CNF formula in a Solver (Part 1)}
  \label{alg:cnf}
\begin{algorithmic}[1]
  \Require{An $(s,k)$-puzzle $P$ and a Solver $S$}
  \Ensure{Solver $S$ with CNF clauses loaded.}
  \Function{ConstructReduction}{$P, S$}
  
  \State{Construct $H_p$}
  \State{Allocate $2s^2$ variables space in Solver $S$}
  \State{Create an empty clause $C$}
  \State{\Comment Adding constraints (variables) from $H_P$}
  \For{$r_1 = 0$ to $s$}             
    \For{$r_2 = 0$ to $s$}
      \State{Get constriant $Var_{r_2}$ from $Var$ with index from $X'$ according to $r_1, r_2$ }
      \For{$r_3 = 0$  to $s$}
	\State{Get constriant $Var_{r_3}$ from $Var$ with index from $Y'$ according to $r_1, r_3$}
        \If {$(r_1, r_2, r_3)$ not in $H_p$}
	  \State{Clear clause $C$}
	  \State{Push $Var_{r_2}$ and $Var_{r_3}$ into $C$}
	  \State{Add clause $C$ into Solver $S$}
        \EndIf
      \EndFor
    \EndFor
  \EndFor
  
  \State{\Comment Checking the uniqueness of the edges in the 3D matching}
  \State{\Comment Within layer $r_1$}
  \State{\Comment For all $r_1$, $r_{21} != r_{22}$, and $r_{31} != r_{32}$ }
  \For{$r_1 = 0$ to $s$}
      \For{$a = 0$ to $s$}
      \State{Get constriant $Var_{r_{21}}$ from $Var$ with index from $X'$ according to $a, r_1$}
      \State{Get constriant $Var_{r_{31}}$ from $Var$ with index from $Y'$ according to $a, r_1$}
      	\For{$b = 0$ to $s$}
        \State{Get constriant $Var_{r_{22}}$ from $Var$ with index from $X'$ according to $b, r_1$}
        \State{Get constriant $Var_{r_{32}}$ from $Var$ with index from $Y'$ according to $b, r_1$}
        \If {$r_{21} < r_{22}$}  \Comment implies $r_{31} < r_{32}$
	  \State{Clear clause $C$}
	  \State{Push $Var_{r_{21}}$ and $Var_{r_{22}}$ into $C$}
	  \State{Add clause $C$ into Solver $S$}
	  \State{Clear clause $C$}
	  \State{Push $Var_{r_{31}}$ and $Var_{r_{32}}$ into $C$}
	  \State{Add clause $C$ into Solver $S$}
        \EndIf
        \EndFor
      \EndFor
  \EndFor
  
  \State{\Comment Checking the uniqueness of the edges in the 3D matching}
  \State{\Comment Within slice $r$}
  \State{\Comment For all $r_2, r_3$, $r_{11} != r_{12}$ }
  \For{$r = 0$ to $s$}
      \For{$r_{11} = 0$ to $s$}
      \State{Get constriant $Var_{r_{21}}$ from $Var$ with index from $X'$ according to $r_{11},r$}
      \State{Get constriant $Var_{r_{31}}$ from $Var$ with index from $Y'$ according to $r_{11},r$}
      	\For{$r_{12} = 0$ to $s$}
        \State{Get constriant $Var_{r_{22}}$ from $Var$ with index from $X'$ according to $r_{12}, r$}
        \State{Get contsriant $Var_{r_{32}}$ from $Var$ with index from $Y'$ according to $r_{12}, r$}
        \If {$r_{21} < r_{22}$}  \Comment implies $r_{31} < r_{32}$
	  \State{Clear clause $C$}
	  \State{Push $Var_{r_{21}}$ and $Var_{r_{22}}$ into $C$}
	  \State{Add clause $C$ into Solver $S$}
	  \State{Clear clause $C$}
	  \State{Push $Var_{r_{31}}$ and $Var_{r_{32}}$ into $C$}
	  \State{Add clause $C$ into Solver $S$}
        \EndIf
        \EndFor
      \EndFor
  \EndFor
  \algstore{bkbreak}
  %\EndFunction
\end{algorithmic}
\end{algorithm}

\addtocounter{algorithm}{-1}
\begin{algorithm}[h]
  \caption{: Create CNF formula in a Solver (Part 2)}
\begin{algorithmic}[1]
\algrestore{bkbreak}

  %\Comment Checking Existence of edges in layer $r_1$
  \For{$r_1 = 0$ to $s$} \Comment Checking Existence of edges in layer $r_1$
    \State{Clear clause $C$}
    \For{$r_2 = 0$ to $s$}
        \State{Get constriant $Var_{r_{2}}$ from $Var$ with index from $X'$ according to $r_{1},r_2$}
        \State{Push $Var_{r_{2}}$ into $C$}
    \EndFor
    \State{Add clause $C$ into Solver $S$}
    
    \State{Clear clause $C$}
    \For{$r_3 = 0$ to $s$}
        \State{Get constriant $Var_{r_{3}}$ from $Var$ with index from $Y'$ according to $r_{1},r_3$}
        \State{Push $Var_{r_{3}}$ into $C$}
    \EndFor
    \State{Add clause $C$ into Solver $S$}
  \EndFor
  \For{$r = 0$ to $s$} \Comment Checking Existence of edges in slice $r$
    \State{Clear clause $C$}
    \For{$r_1 = 0$ to $s$}
        \State{Get constriant $Var_{r_{x}}$ from $Var$ with index from $X'$ according to $r_{1},r$}
        \State{Push $Var_{r_{x}}$ into $C$}
    \EndFor
    \State{Add clause $C$ into Solver $S$}
    
    \State{Clear clause $C$}
    \For{$r_1 = 0$ to $s$}
        \State{Get constriant $Var_{r_{y}}$ from $Var$ with index from $Y'$ according to $r_{1},r$}
        \State{Push $Var_{r_{y}}$ into $C$}
    \EndFor
    \State{Add clause $C$ into Solver $S$}
  \EndFor
  \State{Clear clause $C$}
  \For{$r_1 = 0$ to $s$} \Comment check non-trivial cases
  	\State{Get constriant $Var_{rxx}$ from $Var$ with index from $X'$ according to $r_{1},r_1$}
	\State{Get constriant $Var_{ryy}$ from $Var$ with index from $Y'$ according to $r_{1},r_1$}
  \EndFor
  \State{Add clause $C$ into Solver $S$}
  \State{\Return{ Solver $S$}}
  \EndFunction


\end{algorithmic}
\end{algorithm}

 
\subsection{3DM to MIP}

Another way to utilize the connection between verification of strong USP and 3D matching is to furthe reduce 3D matching into an integer programming problem, which is another commonly faced Np optimization problem. By definition(I didn't see a definition number for 3D matching), we know that a tripartite 3-hypergraph $G$ has a \emph{3D matching} if there exists a subset $M \sse E$ with $|M| = |P|$ and for all distinct
hyperedges $e_1, e_2 \in M$, $e_1$ and $e_2$ are \emph{vertex disjoint}. So if we define $f(e) = 1$ for all hyperedge $e \in G$ and $f(e) = 1$ for all hyperedge $e \notin G$, we can say $G$ has a \emph{3D matching}  if there exists a subset $M$ that satisfy the following Constrains: $\sum_{i,j = 1}^{s} f((i,j,k)) \forall k \in [0..s] = \sum_{i,k = 1}^{s} f((i,j,k)) \forall j \in [0..s] = \sum_{j,k = 1}^{s} f((i,j,k))\forall i \in [0..s] = 1 $ for hyperedge $(i,j,k) \in M$. In addition, we can detect a 3D matching is nontrivial by adding the constrain $\sum_{r = 0}^{s} f(r,r,r) < s$.

By this theory, \autoref{alg:mip} would first transfers input puzzle $P$ into a tripartite hypergraph $H_p$ and creates an empty mixed integer programming model. For every hyperedge $e= \notin H_p$, a constrain $f(e) = 0$ is added into the model as it can not be selected into subset $M$. Afterward the algorithm would also add the restrictions listed in the paragraph above (I am not sure if I should list all the constrains again?). Finally, the model is put through a integer programming solver. If the model has a solution, $H_p$ has a nontrivial 3D matching and by \autoref{lem:verify-to-3dm} $P$ is not a USP. Otherwise if the model is infeasible, $H_p$ does not have a nontrivial thus $P$ is a USP.



\begin{algorithm}
  \caption{: Reduction to mixed integer programming}
  \label{alg:mip}
\begin{algorithmic}[1]
  \Require{An $(s,k)$-puzzle $P$.}
  \Ensure{YES, if $P$ is a strong USP and NO otherwise.}
  \Function{VerifyMIP}{$P$}
  \State{Construct $H_p$}
  \State{Construct $model$}
  \For{$r_1 = 0$ to $s$}
    \For{$r_2 = 0$ to $s$}
      \For{$r_3 = 0$  to $s$}
        \If {$(r_1, r_2, r_3)$ not in $H_p$}
          \State{\textsc{AddConstrain}$(f(r_1, r_2, r_3) = 0, model)$}
        \EndIf
      \EndFor
    \EndFor
  \EndFor
  \For {$i = 0$ to $s$}
    \State{\textsc{AddConstrain}$(\sum_{j,k=0}^{s} f(i, j,k) =1, model)$}
    \State{\textsc{AddConstrain}$(\sum_{j,k=0}^{s} f(j, i,k) =1, model)$}
    \State{\textsc{AddConstrain}$(\sum_{j,k=0}^{s} f(j,k, i) =1, model)$}
  \EndFor
  \State{\textsc{AddConstrain}$(\sum_{r=0}^{s} f(r,r,r) < s, model)$}

  \State{$status$ = \textsc{optimize}$(model)$}
  \If{$status == INFEASIBLE$}
    \State{\Return{$True$}}
  \Else
    \State{\Return{$False$}}
  \EndIf

  \EndFunction
\end{algorithmic}
\end{algorithm}

In this algorithm the running time is dominated by the function \textsc{AddConstrain} and \textsc{optimize}. However as we used a third-party optimizer Gurobi for solving integer programming problem, the exact run time for these two function cannot be known. Instead, we can estimate the performance for \textsc{AddConstrain} as $add(1)$ as it only add a new constrain to the model.The performance for \textsc{optimize} is assumed to be $opt(N_c)$ where $N_c$ is the number of constrains. As number of constrains is dependent on $s$ and it ranges from $s^3+1$ to $2s^3+1$, we can also show the run time for \textsc{optimize} as $opt(s^3)$. Base on these assumptions, we can conclude that the performance for \autoref{alg:mip} is $O(T_{3DM}(s,k) + s^3 \cdot add(1) +  opt(s^3))$ as $T_{3DM}(s,k)$ take in account of time to construct $H_p$. The time spend on constructing a new $model$ is constant and thus ignored.

\label{subsec:mip}




\subsection{Heuristics}
\label{sec:heuristic}

Despite our efforts devise an exact algorithm for verifying strong
USPs, the performance of the final algorithm was not practical.  To
speed the algorithm up we considered a number of verification
heuristics that were fast to evaluate, but did not alway produce a
definitive answer.  The heuristics output YES, NO, or MAYBE to whether
a given puzzle $P$ was a strong USP.  We consider only Las Vegas
algorithms here, that is, algorithms whose output of YES or NO is
always correct.  Further, all the heuristics we consider are one-sided
in that they for all input they either return YES or MAYBE, or NO or
MAYBE.  To verify a puzzle $P$ we run a battery of fast heuristics and
return early if any of the heuristics produce a definitive YES or NO.
When all the heuristics result in MAYBE then run one of the slower
exact algorithms that were discussed in the previous section.  Most of
the heuristics we considered were deterministic, but a few were
randomized.  This is again in the Las Vegas sense: An output of YES or
NO is always correct, but as function of the input and internal
algorthmic randomness MAYBE can be also be output and can vary between
independent runs.  The heuristics have several different forms, but
all rely ultimately on structural properties of strong USP.

\subsubsection{Downward Closed}

The simplest heuristics we considered were based on the fact that
strong USP are downward closed, that is, if $P$ is a strong USP, then
so is every subpuzzle $P' \sse P$.  This leads to practical heuristic
that can determine that a puzzle is not a strong USP.

\begin{algorithm}
  \caption{: Downward-closed Heuristic}
  \label{alg:downward-closed}
\begin{algorithmic}[1]
  \Require{An $(s,k)$-puzzle $P$, and size $s' \le s$.}
  \Ensure{NO, if $P$ has a set of $s'$ rows that do not form a strong USP, and MAYBE otherwise.}
  \Function{HeuristicDownwardClosed}{$P, s'$}
  \For{$P' \sse P, |P'| = s'$}
      \If{$P'$ is not a strong USP} \Return{NO.} \EndIf
  \EndFor{}
  \State{\Return{MAYBE}.}
  \EndFunction
\end{algorithmic}
\end{algorithm}

This algorithm runs in time $O(s^{s'} T_{Verify}(s', k))$ where
$T_{Verify}(s',k)$ is the running time for exactly verifying a
$(s',k)$-puzzle.  In practice we did not apply this heuristic for $s'$
larger than $3$, so the effective running time was $O(s^3 T(3,k))$,
which is polynomial in $s$ and $k$ using the verification algorithms
from the previous section which eliminate dependence on $k$ for
polynomial cost.  This heuristic can be made even more practical by
caching the results for puzzles of size $s'$, reducing the
verification time per iteration to constant in exchange for
$O(2^{s'k}T(s',k))$ time to precompute the values for all puzzles of
size $s'$.  There is also space overhead because the precomputed
results are being cached.  We also note that for a puzzle $P$ that is
a strong USP, the heuristic takes $\Theta(s^3 T(3,k))$ time without
cache or $\Theta(s^3)$ with caching.

Note that since our search algorithms typically start from a known
strong USP and tries to add rows, looking a subsets of a puzzle that
have already been verified is wasteful -- these are skipped over in
this setting.  From a practical point of view, running this heuristic
is free for small constant $s'$, as the exact verification algorithms
have a matching or higher polynomial running time.  Furthermore, since
the algorithm can return early, its expected running time on random
non-strong USPs is low.  There appeared to be dimenishing returns with
increasing $s'$ as it substantially increases precompute time and
storage while each stored value contributes relatively less to
verification as there are now many more values stored.

\subsubsection{Random}

XXX - There's also a random reordering heuristic, but I don't remember
the justification for it.  It's more elaborate than the other, but
less motivated.

\begin{algorithm}
  \caption{: Random Heuristic}
  \label{alg:random}
\begin{algorithmic}[1]
  \Require{An $(s,k)$-puzzle $P$, and iteration bound $t$.}
  \Ensure{NO, if a witness is found for $P$ not being a strong USP, and MAYBE otherwise.}
  \Function{HeuristicRandom}{$P$}
  \State{XXX - Fill out.}
  \EndFunction
\end{algorithmic}
\end{algorithm}

\subsubsection{Greedy}

This heuristic attempts to solve the 3D-Matching instance associated
with verifying a puzzle $P$ (discussed in \autoref{sec:3DM}).  The
heuristic proceeds iteratively, determining the layer of the 3DM
instance with the least edges and randomly selecting an edge in that
layer to put into the 3DM.  If the heuristic successfully contructs a
3DM it returns NO indicating that the input puzzle $P$ is not a strong
USP.  If the heuristic reaches point were prior commits have made the
matching infeasible, the heuristic starts again from scratch.  This
process is repeated some number of times before it gives up and
returns MAYBE.  In our practical implementation we use $s^3$
iterations because the time balances well with the other heuristics
and effectively reduced the number of instances requiring a full
verification.

\begin{algorithm}
  \caption{: (Random) Greedy Heuristic}
  \label{alg:random-greedy}
\begin{algorithmic}[1]
  \Require{An $(s,k)$-puzzle $P$, and iteration bound $t$.}
  \Ensure{NO, if a witness is found for $P$ not being a strong USP, and MAYBE otherwise.}
  \Function{HeuristicGreedy}{$P$}
  \State{Construct 3DM instance $G : [s] \times [s] \times [s] \rightarrow \set{0,1}$ using $P$.}
  \For{$i = 1$ to $t$}
    \For{$j = 1$ to $s$}
      \State{$counts[j] =$ Number of edges incident vertex $j = \sum_{q,r} G(j,q,r)$.}
    \EndFor
    \State{Let $J,Q,R = \emptyset.$}
    \State{$m = 1.$}
    \While{$m \le s$}
      \State{\texttt{// Select edge to add.}}
      \State{Select $j \in \condset{j \in \bar{J}}{counts[j] = \max_{q \in \bar{J}} counts[q]}$ uniformly at random.}
      \If{$counts[j] \le 0$} break. \EndIf
      \State{Select $(q,r) \in \condset{(q,r) \in \bar{Q} \times \bar{R}}{G(j,q,r) = 1}$ uniformly at random.}
      \State{\texttt{// Update edges counts.}}
      \For{$u = 1$ to $s$}
        \For{$v = 1$ to $s$}
          \If{$(u,v) \in \bar{Q} \times \bar{R}$ and $G(j,u,v) = 1$} $counts[j] = counts[j] - 1$. \EndIf
          \If{$(u,v) \in \bar{J} \times \bar{R}$ and $G(u,q,v) = 1$ and $u \neq j$} $counts[u] = counts[u] - 1$. \EndIf
          \If{$(u,v) \in \bar{J} \times \bar{Q}$ and $G(u,v,r) = 1$ and $u \neq j$ and $v \neq q$} $counts[u] = counts[u] - 1$. \EndIf
        \EndFor
      \EndFor
      \State{$J = J \cup \set{j}$.}
      \State{$Q = Q \cup \set{q}$.}
      \State{$R = R \cup \set{r}$.}
      \State{$m = m + 1.$}
      \EndWhile
    \If {$m > s$} \Return{NO}. \EndIf
  \EndFor
  \State{\Return{MAYBE}.}
  \EndFunction
\end{algorithmic}
\end{algorithm}

The array $counts$ is used to store the number of edges $counts[j]$
that remain associated with vertex $j$ along the first coordinate.
Much of the algorithm is devoted to maintaining this invariant.  The
arrays $J,Q,R$ store the vertices along the three coordinates,
respectively, that have already been incorporated into the partial 3D
matching.  Like in our previous algorithms we do not store the
matching itself, only the vertices involved.  The break at Line 10
triggers when the partial 3D matching is a dead end and cannot be
extended into a full 3D matching.  The condition of Line 22 is true
when a full 3D matching has been constructed and causes the algorithm
to return that $P$ is not a strong USP.

The running time of this algorithm is $O(s^3 t + T_{3DM}(s,k))$, where
$T_{3DM}(s,k)$ is the time required to construct 3D matching
instances from $(s,k)$-puzzles.  This algorithm has the potential to
be considerably slower than the downward-closure heuristic, especially
when $t = s^3$.  However, the main loop can terminate early at Line 10
when it fails to extend the 3D matching this permits the expected time
to much less than the worst case.  For a puzzle $P$ that is a strong
USP, the heuristic takes the full $\Omega(s^3 t + T_{3DM}(s,k))$ time.


\begin{comment}


\subsubsection{Graph Automorphism}

A strong uniquely-solvable puzzle must also be a uniquely-solvable
puzzle.  Given a puzzle $P$ we can construct a graph $G_P$ such that
$G_P$ is rigid iff $P$ is a uniquely-solvable puzzle.

XXX - I don't think this idea worked out.  The code correctly
implemented the approach, but the approach was flawed.  Probably
remove this section or add to future work.

\end{comment}

\subsubsection{2D Matching}

The final heuristic we present is one-sided in the opposite direction
of the others, it may return YES or MAYBE.  In order for a hypergraph
$G = \langle [s] \times [s] \times [s], E\rangle$ to have a 3D
matching it must be the case that when any one of the three parts of
vertices of $G$ is projected away the resulting bipartite graph has a
perfect matching.  Thus we can witness that there is no 3D matching by
determining that one of three projected bipartite graphs $G|_1, G|_2,
G|_3$ does not have a perfect matching.

\begin{algorithm}
  \caption{: 2D Matching Heuristic}
  \label{alg:2dm}
\begin{algorithmic}[1]
  \Require{An $(s,k)$-puzzle $P$.}
  \Ensure{YES, if $P$ is found to be strong USP, and MAYBE otherwise.}
  \Function{Heuristic2DMatching}{$P$}
  \State{Construct 3DM instance $G : [s] \times [s] \times [s] \rightarrow \set{0,1}$ using $P$.}
  \State{\texttt{// Construct projections.}}
  \State{Define projection $G|_1(b,c) = \vee_{a \in [s]} G(a,b,c)$.}
  \State{Define projection $G|_2(a,c) = \vee_{b \in [s]} G(a,b,c)$.}
  \State{Define projection $G|_3(a,b) = \vee_{c \in [s]} G(a,b,c)$.}

  \If{$G|_1, G|_2,$ and $G|_3$ have bipartite perfect matchings}
  \State{\Return{MAYBE.}}
  \Else
  \State{\Return{YES.}}
  \EndIf
  \EndFunction
\end{algorithmic}
\end{algorithm}

We can efficiently decide bipartite perfect matching using the
standard reduction to network flow and solve it using the
Ford-Faulkerson augmenting flow algorithm.  This approach yields an
algorithm that runs in time $O(s^3 + T_{3DM}(s,k))$, where
$T_{3DM}(s,k)$ is the time required to construct 3D matching instances
from $(s,k)$-puzzles.

In practice we found this heuristic not to be effective because the
projected $G|_i$ are typically dense graphs even if $G$ is not, and so
they are likely to have perfect matchings independently of whether $G$
has a 3D matching.



\subsection{Hybrid Algorithm}

Our final verification algorithm is a hybrid of several verification
algorithms and heuristics.  The size thresholds for which algorithm
and heuristic to apply were determined experimentally for small $k$
and were focused on the values were our strong USP search algorithms
were tractable $k \le 6$ (or nearly tractable $k \le 8$).  We decided
to run both the reduction to SAT and MIP in parallel because it was
not clear which algorithm performed better.  Since verification halts
when either algorithm completes the wasted effort is within a factor
of two of what the better algorithm could have done alone.  We also
chose to do this because we experimentally observed that there were
many instances that one of the algorithms struggled with that the
other did not.  We end up not using \textsc{Heuristic2DMatching}
because it rarely produced definitive output.

\begin{algorithm}
  \caption{: Final Hybrid Verification Algorithm}
  \label{alg:hybrid}
\begin{algorithmic}[1]
  \Require{An $(s,k)$-puzzle $P$.}
  \Ensure{YES, if $P$ is found to be strong USP, and MAYBE otherwise.}
  \Function{Verify}{$P$}
  \If{$s \le 2$} \Return{\Call{VerifyBruteForce}{$P$}.} \EndIf
  \If{$s \le 7$} \Return{\Call{VerifyBidirectional}{$P$}.} \EndIf
  \If{$s \le 10$}
    \State{Return result if \Call{HeuristicDownwardClosed}{$P, 2$} is not MAYBE.}
    \State{\Return{\Call{VerifyBidirectional}{$P$}.}}
    \EndIf
  \State{Return result if \Call{HeuristicDownwardClosed}{$P, 3$} is not MAYBE.}
  \State{Return result if \Call{HeuristicRandom}{$P$} is not MAYBE.}
  \State{Return result if \Call{HeuristicGreedy}{$P$} is not MAYBE.}
  \State{Run \Call{VerifySAT}{$P$} and \Call{VerifyMIP}{$P$} in parallel and return the first result.}
  \EndFunction
\end{algorithmic}
\end{algorithm}

\section{Searching for Strong USPs}
\label{sec:search}

In certain respects, the problem of constructing a large strong USP is
much like the problem of constructing a large set of linearly
independent vectors in a vector space.  Indeed, the object to be
constructed is a set and the order which elements are added does not
matter, further the underlying elements are represented as a vector.
There are polynomial-time algorithms for determining whether a set of
vectors are independent, and we have a practical algorithm for
deciding whether a puzzle is a strong USP.

There is a straightforward algorithm for constructing maxmimum size
sets of independent vectors: Start with an empty set $S$, and
repeatedly add vectors to $S$ which are linearly independent of the
vectors currently in $S$.  After this process completes, $S$ is a
largest set of linearly independent vectors.  This problem admits such
a greedy algorithm because the sets of linearly independent vectors
form a matroid.  The vector to be added each step can be computed
efficiently by solving a linear system of equations for vectors in the
null space of $S$.  Altogether this gives an efficient algorithm to
compute a maximum set of independent vectors or, more generally, to
complete a maximum set with some given subset $S'$.

Unfortunately this same approach does not work for generating maximum
size strong USPs.  The set of strong USP do not form a matroid, rather
they only form an independence system \cite{XXX}.  In particular, the
empty puzzle is a strong USP and the set of strong USPs are downward
closed, so that if $P_2$ is a strong USP and $P_1 \subseteq P_2$, then
$P_1$ is a strong USP.  The third and final property required to be a
matroid, the augmentation property, requires that for every pair of
strong USPs $P_1, P_2$ with $|P_1| \le |P_2|$ there is a row of $r \in
P_2 \backslash P_1$ such that $P_1 \cup \set{r}$ is also a strong USP.
A simple counterexample with the strong USPs $P_1 = \set{32}$ and $P_2
= \set{12, 23}$ concludes that neither $P_1 \cup \set{12} = \set{12,
  32}$ nor $P_1 \cup \set{23} = \set{23, 32}$ are strong USP, and
hence the augmentation property does not hold.

One consequence is that wholly greedy algorithms will not be effective
for finding maximum size strong USPs \cite{XXX}.  Furthermore, we do not
currently have a practical or efficient algorithm that can take a
strong USP $P$ and determine a single row, or set of rows, that can
extend $P$ while maintaining that it is a strong USP.

That said, we have had some success applying general purpose search
techniques together with our practical verification algorithm to
construct maximum size strong USPs for small $k$.  In particular, we
implemented variants of depth-first search (DFS) and breadth-first
search (BFS) in both desktop and HPC settings.

A width-$k$ puzzle has potentially $3^k$ distinct rows.  For $j \in
[3^k]$ we use $r_j$ to denote the $j^{th}$ row in lexicographic order.
For a puzzle $P$ and integer $i \in [|P|]$ we use $P_i$ to denote the
$i^{th}$ row of $P$ where the rows are sequenced in lexicographic
order. We include pseudocode of both searches for completeness.

\begin{algorithm}
  \caption{: Depth-First Search}
  \label{alg:dfs}
\begin{algorithmic}[1]
  \Require{An integer $k \ge 0$, and a width-$k$ strong USP $P$.}
  \Ensure{The number $best$, which is the size of the largest strong USP that has $P$ as a subpuzzle.}

  \Function{DFS}{$P$}

  \State{Let $best = |P|$.}
  \State{Let $j$ be the index of the row $P_{|P|-1}$ in lexicographic order.}

  \For{$r = r_{j+1}$ to $r_{3^k-1}$}
    \State{Let $P' = P \cup \set{r}$.}
    \If{\Call{Verify}{$P'$}}
      \State{$best = \max(best, $ \Call{DFS}{$k, P'$}).}
    \EndIf
  \EndFor

  \State{\Return{$best$}.}

  \EndFunction
\end{algorithmic}
\end{algorithm}

\begin{algorithm}
  \caption{: Breadth-First Search}
  \label{alg:bfs}
\begin{algorithmic}[1]
  \Require{An integer $k \ge 0$.}
  \Ensure{The number $best$, which is the size of the largest strong USP that has $P$ as a subpuzzle.}

  \Function{BFS}{$k$}

  \State{Let $Q$ be an empty queue.}
  \State{Push $\emptyset$ onto end of $Q$.}

  \While{$Q$ is not empty}
    \State{Pop head of $Q$ into $P$.}
    \State{Let $j$ be the index of the row $P_{|P|-1}$ in the lexicographic order of all rows.}
    \For{$r = r_{j+1}$ to $r_{3^k-1}$}
      \State{Let $P' = P \cup \set{r}$.}
      \If{\Call{Verify}{$P'$}}
        \State{Push $P'$ onto end of $Q$.}
        \State{$best = |P'|$.}
      \EndIf
    \EndFor
  \EndWhile

  \State{\Return{$best$}.}

  \EndFunction
\end{algorithmic}
\end{algorithm}

Arguing the correctness of these algorithms is routine.  It should be
noted that since we treat the puzzles as though they have rows sorted
in lexicographic order, we only need to consider adding rows that
occur later in the order than the highest row already in the puzzle,
which is $r_j$.  This explains the initial row $r_{j+1}$ being used in
the for loop in both algorithms.  Observe that \Call{BFS}{$k$} $=$
\Call{DFS}{$k, \emptyset$} is the maximum size of a strong USP of
width $k$.  The worst case running time of these algorithms is
similarly routine, they both run in time $O(3^k \cdot \#USP(k) \cdot
T_{\textsc{Verify}}($\Call{BFS}{$k}$, $k))$ where $\#USP(k)$ is the
number of strong USPs of width $k$ and
$T_{\textsc{Verify}}(BFS(k)+1,k)$ is the time to verify the maximum
size puzzle examined by the algorithm (assuming $T_{\textsc{Verify}}$
is monotone in its parameters).

The actual running times of both these algorithms are prohibitive even
for $k = 5$, and the greater memmory usage of \textsc{BFS} to store
the entire search frontier in $Q$ is in the TBs for $k = 6$.  There
are some silver linings, \textsc{DFS} can report intermediate results
which are the maximal strong USPs that it has discovered so far.  Both
algorithms admit the possibility of eliminating puzzles from the
search that are equivalent to puzzles that have already been searched,
though it is easier to fit into the structure \textsc{BFS} as the
puzzles are already being stored in a queue.

\begin{comment}
  Not planning to discuss:
  \begin{itemize}
  \item A$^*$ + admissible heuristics.
  \item Upper bounds from A$^*$.
  \item Symmetry removal.
  \end{itemize}
\end{comment}

\section{Experimental Results}
\label{sec:results}

Our experimental results come in three flavors for small constant
values of $k$: (i) constructive lower bounds on the maximum size of
width-$k$ strong USP, (ii) exhaustive, but nonconstructive, upper
bounds on the maximum size of width-$k$ strong USP, and (iii)
experimental run times comparing the various algorithms for verifying
width-$k$ strong USP.  Here constructive means that specific concrete
strong USPs are produced by our implementation that witness the bound.
\textsc{BFS} and \textsc{DFS} when able to run to completion provide
result of form (i) and (ii) which are tight.  When unable to run to
completion \textsc{DFS} provides only result of form (i), and, as
such, are not guaranteed to be tight.

\subsection{New Bounds on the Size of Strong USPs}
\label{subsec:usps_found}

We begin with a diagram, \autoref{fig:examples}, of representative
examples of maximal strong USPs we found for $k \le 6$.  For space
considerations, we omit larger examples found from this article,
though they are included with the source code of this project.

\begin{figure}
  \label{fig:examples}
  \begin{multicols}{4}
  (1,1):\\[.5ex]
  \begin{tabular}{|c|}
    \hline
    1 \\ \hline
  \end{tabular}\\[6ex]

  (2,2):\\[.5ex]
  \begin{tabular}{|c|c|}
    \hline
    1&3 \\ \hline
    2&1 \\ \hline
  \end{tabular}\\[16ex]

  (3,3):\\[.5ex]
  \begin{tabular}{|c|c|c|}
    \hline
    1&1&1 \\ \hline
    3&2&1 \\ \hline
    3&3&2 \\ \hline
    \end{tabular}\\[2ex]

  (5,4):\\[.5ex]
  \begin{tabular}{|c|c|c|c|}
    \hline
    3&1&3&2 \\ \hline
    1&2&3&2 \\ \hline
    1&1&1&3 \\ \hline
    3&2&1&3 \\ \hline
    3&3&2&3 \\ \hline
  \end{tabular}\\[10ex]

  (8,5):\\[.5ex]
  \begin{tabular}{|c|c|c|c|c|}
    \hline
    3&3&3&1&1 \\ \hline
    1&1&2&2&1 \\ \hline
    2&1&3&3&2 \\ \hline
    3&2&2&2&3 \\ \hline
    2&1&2&1&3 \\ \hline
    2&2&3&1&2 \\ \hline
    3&2&3&2&1 \\ \hline
    3&1&2&1&1 \\ \hline
  \end{tabular}\\[16ex]

  (14,6):\\[.5ex]
  \begin{tabular}{|c|c|c|c|c|c|}
    \hline
    2&3&3&1&1&1 \\ \hline
    2&1&1&2&1&1 \\ \hline
    3&3&1&2&1&1 \\\hline
    3&2&2&2&1&1 \\\hline
    2&3&1&1&2&1 \\\hline
    2&2&3&1&2&1 \\\hline
    3&3&1&3&2&1 \\\hline
    3&2&3&3&2&1 \\\hline
    2&1&1&3&1&2 \\\hline
    2&3&1&3&2&2 \\\hline
    3&1&1&1&1&3 \\\hline
    3&3&2&3&1&3 \\\hline
    3&3&2&1&2&3 \\\hline
    2&2&3&2&2&3 \\\hline
  \end{tabular}
  \end{multicols}
  \caption{Respresentative examples of the largest strong uniquely
    solvable $(s,k)$-puzzles found from width $k = 1$ to $6$.}
\end{figure}

\autoref{table:compare} summarizes our main results which are improved
bounds on the maximum size of strong uniquely solvable puzzles and
compares them to bounds from \cite{cksu05}.  In this table the
(constructive) lower bounds and upper bounds are derived from XXX and
XXX in \cite{cksu05} respectively.  Observe that our experiment
derives tight bounds for all $k \le 5$ and improves the known lower
bounds for $k \le 9$.  The column labelled $\omega^*$ denotes the
upper bound on $\omega$ the strong USP imply when taking the direct
product with itself finitely mainly times. (XXX - Restructure to write
as lemma taking fixed size strong USP to $\omega$.)
\begin{table}
  \label{table:compare}
  \begin{center}
  \begin{tabular}{|c|r|r|r|r|}
    \hline
    & \multicolumn{2}{|c|}{[CKSU05]} & \multicolumn{2}{|c|}{This work} \\
    \hline
    $k$ & Maximum $s$ & $\omega^*$~ & Maximum $s$ & $\omega^*$~\\
    \hline
    1 & $s\le ~~~~1$ & & $1=s$ & $3.000$  \\
    2 & $s\le ~~~~3$ & & $2=s$ & $2.670$ \\
    3 & $3 \le s \le ~~~~6$ & $2.642$ & $3=s$ & $2.642$ \\
    4 & $s\le ~~12$ & & $5=s$ & $2.585$ \\
    5 & $s\le ~~24$ & & $8=s$ & $2.562$  \\
    6 & $10 \le s \le ~~45$ & $2.615$ &$14\le s$ & $2.521$\\
    7 & $s\le ~~86$ & & $21\le s$ & $2.531$ \\
    8 & $s\le 162$ & & $30\le s$ & $2.547$ \\
    9 & $36 \le s \le 307$ & $2.592$ &$42\le s$ & $2.563$  \\
    \hline
  \end{tabular}
  \end{center}
  \caption{Comparison of bounds on maximum size of strong uniquely
    solvable $(s,k)$-puzzles with \cite{cksu05}.  $\omega^*$ is the
    $\omega$ in the limit of composing puzzles of these dimensions via
    direct product.}
\end{table}

We conjecture that $s = 14$ is tight for $k = 6$, given the amount of
CPU cycles and different algorithmic approaches we have applied to
this case where it quickly, on order of an hour of CPU time, gets to
$14$, and makes no further progress in the succeeding month.  We do
not have any strong beliefs about the lower bounds for larger values
of $k$ except that they are likely not tight.  Furthermore, it is
appearant that exhaustive search will not be practical for $k \ge 6$
unless the search space can be substantially reduced.

XXX - generate, add, and discuss data on search space size, perhaps
after removing symmetries.


\subsection{Algorithm Performance}
\label{subsec:performance}

XXX - Tables / plots of data on verify's performance.

XXX - Tables / plots of data on search performance?

\section{Conclusions}
\label{sec:conclusion}

We initiated the first study of the verification strong USP and
developed practical software for both verifying and search for strong
USP.  We give tight results on the maximum size of width-$k$ strong
USP for $k \le 5$.  Although our results do not produce a new upper
bound on the running time of matrix multiplication, they demonstrate
there is promise in the uniquely solvable puzzle approach.  The
immediate open question is: What are tight bounds on maximum size
strong USP for $k \ge 6$ and do these bound lead to asymptotically
faster algorithms for matrix multiplication?  The main bottleneck in
our work is the size of the search space.  This leads to a number of
open questions: Can the size of the search space be lowered by
additional reductions in symmetry?  We have some preliminary data that
suggests this is possible, but is not sufficient to make exhaustive
search width-6 strong USP feasible.  Is there a way to identify rows
that can be added to a strong USP other than examining all possible
rows and checking whether the puzzle remains a strong USP?  Are there
other search strategies that would be more effective on this search
space?  Is strong USP verification \NP-complete?

\bibliographystyle{customurlbst/alphaurlpp} \bibliography{references}

\appendix


\end{document}
