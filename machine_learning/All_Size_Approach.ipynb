{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.2133e+04 2.3321e+04 3.2133e+04 2.3333e+04 4.4000e-01 3.2000e-01\n",
      " 2.4000e-01 0.0000e+00 0.0000e+00 1.0000e+00 0.0000e+00 1.0000e+00\n",
      " 0.0000e+00 0.0000e+00 1.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00\n",
      " 1.0000e+00 0.0000e+00 1.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00\n",
      " 0.0000e+00 0.0000e+00 0.0000e+00 1.0000e+00 0.0000e+00 1.0000e+00\n",
      " 0.0000e+00 0.0000e+00 0.0000e+00 1.0000e+00 1.0000e+00 0.0000e+00\n",
      " 0.0000e+00 1.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00\n",
      " 0.0000e+00 0.0000e+00 1.0000e+00 0.0000e+00 1.0000e+00 0.0000e+00\n",
      " 0.0000e+00 1.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00 1.0000e+00\n",
      " 0.0000e+00 0.0000e+00 0.0000e+00 1.0000e+00 0.0000e+00 0.0000e+00\n",
      " 0.0000e+00 1.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00 1.0000e+00\n",
      " 0.0000e+00 0.0000e+00 0.0000e+00 1.0000e+00 1.0000e+00 0.0000e+00\n",
      " 0.0000e+00 1.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00\n",
      " 0.0000e+00 0.0000e+00 1.0000e+00 0.0000e+00 1.0000e+00 0.0000e+00\n",
      " 0.0000e+00 1.0000e+00 0.0000e+00 0.0000e+00 1.0000e+00 0.0000e+00\n",
      " 0.0000e+00 1.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00\n",
      " 0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00\n",
      " 0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00\n",
      " 0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00\n",
      " 0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00\n",
      " 0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00\n",
      " 0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00\n",
      " 0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00\n",
      " 0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00\n",
      " 0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00\n",
      " 0.0000e+00 0.0000e+00]\n",
      "[[1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " ...\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]]\n",
      "[0.44 0.32 0.24 0.   0.   1.   0.   1.   0.   0.   1.   0.   0.   0.\n",
      " 1.   0.   1.   0.   0.   0.   0.   0.   0.   1.   0.   1.   0.   0.\n",
      " 0.   1.   1.   0.   0.   1.   0.   0.   0.   0.   0.   0.   1.   0.\n",
      " 1.   0.   0.   1.   0.   0.   0.   1.   0.   0.   0.   1.   0.   0.\n",
      " 0.   1.   0.   0.   0.   1.   0.   0.   0.   1.   1.   0.   0.   1.\n",
      " 0.   0.   0.   0.   0.   0.   1.   0.   1.   0.   0.   1.   0.   0.\n",
      " 1.   0.   0.   1.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      " 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      " 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      " 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      " 0.   0.   0.   0.   0.   0.   0.  ]\n",
      "400000\n",
      "100000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "#from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras import utils\n",
    "import sys\n",
    "\n",
    "\n",
    "#Loads Training Data\n",
    "data =  np.loadtxt('../data/random_canon_r5_c5.csv',dtype = float, delimiter = ',')\n",
    "#names = data[0]\n",
    "#data = data[1:]\n",
    "\n",
    "\n",
    "#Loads Extra SUSP data to increase class percentage\n",
    "extraSUSPs = np.loadtxt('../data/random_canon_SUSPs_r5_c5.csv',dtype = float, delimiter = ',')\n",
    "#extraSUSPs = extraSUSPs[1:]\n",
    "\n",
    "data45 =  np.loadtxt('../data/random_canon_r4_c5.csv',dtype = float, delimiter = ',')\n",
    "extraSUSPs45 = np.loadtxt('../data/random_canon_SUSPs_r4_c5.csv',dtype = float, delimiter = ',')\n",
    "\n",
    "\n",
    "#Creates one dataset\n",
    "data = np.concatenate([data, extraSUSPs])\n",
    "data45 = np.concatenate([data45, extraSUSPs45])\n",
    "\n",
    "data = data[:,1:]\n",
    "\n",
    "data = np.concatenate([data, data45])\n",
    "\n",
    "#Sets class data to y\n",
    "y = data[:,-1]\n",
    "\n",
    "#Choosing which features to include in X data\n",
    "#X = data[:,8:-1]\n",
    "print(data[0])\n",
    "\n",
    "X = data[:,4:-1]\n",
    "\n",
    "\n",
    "#X = X.astype(np.float)\n",
    "y = utils.to_categorical(y) #Done to make categorical loss functions work\n",
    "print(y)\n",
    "\n",
    "print(X[0])\n",
    "\n",
    "#le = preprocessing.LabelEncoder()\n",
    "\n",
    "#for i in range(ROWS+COLUMNS):\n",
    "#    X[:,i] = le.fit_transform(X[:,i])\n",
    "\n",
    "#cat_features = [0, 1, 2, 3 ,4 ,5 ,6 ,7 ,8 ,9]\n",
    "#enc = preprocessing.OneHotEncoder(categorical_features=cat_features)\n",
    "#enc.fit(X)\n",
    "\n",
    "#print(enc.n_values_)\n",
    "#print(enc.feature_indices_)\n",
    "#X = enc.transform(X).toarray()\n",
    "#print(X[0])\n",
    "\n",
    "\n",
    "#Creates a testing and training split that is stratified\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.20, stratify=y)\n",
    "\n",
    "print(len(x_train))\n",
    "print(len(x_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nimport matplotlib.pyplot as plt\\nfrom scipy import stats\\n\\nfrom sklearn import model_selection\\nfrom sklearn.linear_model import LogisticRegression\\nfrom sklearn.tree import DecisionTreeClassifier\\nfrom sklearn.neighbors import KNeighborsClassifier\\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\\nfrom sklearn.naive_bayes import GaussianNB\\nfrom sklearn.svm import SVC\\nfrom sklearn.ensemble import RandomForestClassifier\\nfrom sklearn.neural_network import MLPClassifier\\nfrom sklearn.dummy import DummyClassifier\\nfrom sklearn.feature_selection import RFE\\n\\n\\n# prepare configuration for cross validation test harness\\nseed = 1\\n\\n# prepare models\\nmodels = []\\nmodels.append((\\'ZR\\', DummyClassifier(strategy=\"most_frequent\")))\\nmodels.append((\\'LR\\', LogisticRegression(solver=\\'liblinear\\')))\\n#models.append((\\'KN5\\', KNeighborsClassifier()))  # Too Slow commented out\\n#models.append((\\'KN7\\', KNeighborsClassifier(n_neighbors=7)))\\nmodels.append((\\'DT\\', DecisionTreeClassifier()))\\nmodels.append((\\'NB\\', GaussianNB()))\\n#models.append((\\'SVM\\', SVC(gamma=\\'auto\\')))\\n#models.append((\\'LIN\\', SVC(kernel=\\'linear\\',gamma=\\'auto\\')))\\n#models.append((\\'RF\\',RandomForestClassifier(n_estimators=100)))\\n\\n# evaluate each model in turn\\n# note that I\\'m going to run through each model above\\n# performing a 10-fold cross-validation each time\\n# (n_splits = 10), specifying \\'accuracy\\' as my measure\\n\\nresults = []\\nclassifiers = []\\nscoring = \\'accuracy\\'\\nfor name, model in models:\\n\\tkfold = model_selection.KFold(n_splits=10, random_state=seed)\\n\\tcv_results = model_selection.cross_val_score(model, X, y, cv=kfold, scoring=scoring)\\n\\tresults.append(cv_results)\\n\\tclassifiers.append(name)\\n\\tmsg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\\n\\tprint(msg)\\n\\n    \\n    \\n# boxplot algorithm comparison\\n\\nfig = plt.figure()\\nfig.suptitle(\\'Algorithm Comparison\\')\\nax = fig.add_subplot(111)\\nplt.boxplot(results)\\nax.set_xticklabels(classifiers)\\nplt.show()\\n\\n#print(\\'\\n***Performing t-tests***\\n\\n\\')\\n\\n    \\n#ttest,pval = stats.ttest_rel(results[0], results[1])\\n#print(\\'P-Val between ZeroR and Logistic Regression: %.2f\\' % pval)\\n\\n#if pval<0.05:\\n#    print(\"reject null hypothesis\")\\n#else:\\n#    print(\"accept null hypothesis\") \\n\\n#print()    \\n\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\"\"\"\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "\n",
    "from sklearn import model_selection\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.feature_selection import RFE\n",
    "\n",
    "\n",
    "# prepare configuration for cross validation test harness\n",
    "seed = 1\n",
    "\n",
    "# prepare models\n",
    "models = []\n",
    "models.append(('ZR', DummyClassifier(strategy=\"most_frequent\")))\n",
    "models.append(('LR', LogisticRegression(solver='liblinear')))\n",
    "#models.append(('KN5', KNeighborsClassifier()))  # Too Slow commented out\n",
    "#models.append(('KN7', KNeighborsClassifier(n_neighbors=7)))\n",
    "models.append(('DT', DecisionTreeClassifier()))\n",
    "models.append(('NB', GaussianNB()))\n",
    "#models.append(('SVM', SVC(gamma='auto')))\n",
    "#models.append(('LIN', SVC(kernel='linear',gamma='auto')))\n",
    "#models.append(('RF',RandomForestClassifier(n_estimators=100)))\n",
    "\n",
    "# evaluate each model in turn\n",
    "# note that I'm going to run through each model above\n",
    "# performing a 10-fold cross-validation each time\n",
    "# (n_splits = 10), specifying 'accuracy' as my measure\n",
    "\n",
    "results = []\n",
    "classifiers = []\n",
    "scoring = 'accuracy'\n",
    "for name, model in models:\n",
    "\tkfold = model_selection.KFold(n_splits=10, random_state=seed)\n",
    "\tcv_results = model_selection.cross_val_score(model, X, y, cv=kfold, scoring=scoring)\n",
    "\tresults.append(cv_results)\n",
    "\tclassifiers.append(name)\n",
    "\tmsg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n",
    "\tprint(msg)\n",
    "\n",
    "    \n",
    "    \n",
    "# boxplot algorithm comparison\n",
    "\n",
    "fig = plt.figure()\n",
    "fig.suptitle('Algorithm Comparison')\n",
    "ax = fig.add_subplot(111)\n",
    "plt.boxplot(results)\n",
    "ax.set_xticklabels(classifiers)\n",
    "plt.show()\n",
    "\n",
    "#print('\\n***Performing t-tests***\\n\\n')\n",
    "\n",
    "    \n",
    "#ttest,pval = stats.ttest_rel(results[0], results[1])\n",
    "#print('P-Val between ZeroR and Logistic Regression: %.2f' % pval)\n",
    "\n",
    "#if pval<0.05:\n",
    "#    print(\"reject null hypothesis\")\n",
    "#else:\n",
    "#    print(\"accept null hypothesis\") \n",
    "\n",
    "#print()    \n",
    "\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num of Features:  147\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 192)               28416     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 192)               37056     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 192)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 192)               37056     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 192)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 192)               37056     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 192)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 192)               37056     \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 192)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 192)               37056     \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 192)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 2)                 386       \n",
      "=================================================================\n",
      "Total params: 214,082\n",
      "Trainable params: 214,082\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 320000 samples, validate on 80000 samples\n",
      "Epoch 1/200\n",
      "320000/320000 [==============================] - 15s 47us/step - loss: 0.5029 - acc: 0.7274 - val_loss: 0.4387 - val_acc: 0.7672\n",
      "Epoch 2/200\n",
      "320000/320000 [==============================] - 14s 43us/step - loss: 0.4126 - acc: 0.7811 - val_loss: 0.3787 - val_acc: 0.8009\n",
      "Epoch 3/200\n",
      "320000/320000 [==============================] - 14s 43us/step - loss: 0.3713 - acc: 0.8071 - val_loss: 0.3522 - val_acc: 0.8171\n",
      "Epoch 4/200\n",
      "320000/320000 [==============================] - 14s 43us/step - loss: 0.3474 - acc: 0.8203 - val_loss: 0.3391 - val_acc: 0.8276\n",
      "Epoch 5/200\n",
      "320000/320000 [==============================] - 14s 44us/step - loss: 0.3323 - acc: 0.8304 - val_loss: 0.3230 - val_acc: 0.8345\n",
      "Epoch 6/200\n",
      "320000/320000 [==============================] - 13s 41us/step - loss: 0.3208 - acc: 0.8373 - val_loss: 0.3111 - val_acc: 0.8403\n",
      "Epoch 7/200\n",
      "320000/320000 [==============================] - 13s 41us/step - loss: 0.3134 - acc: 0.8418 - val_loss: 0.3052 - val_acc: 0.8446\n",
      "Epoch 8/200\n",
      "320000/320000 [==============================] - 14s 43us/step - loss: 0.3063 - acc: 0.8466 - val_loss: 0.3015 - val_acc: 0.8472\n",
      "Epoch 9/200\n",
      "320000/320000 [==============================] - 14s 43us/step - loss: 0.2999 - acc: 0.8504 - val_loss: 0.3023 - val_acc: 0.8457\n",
      "Epoch 10/200\n",
      "320000/320000 [==============================] - 14s 43us/step - loss: 0.2942 - acc: 0.8543 - val_loss: 0.2948 - val_acc: 0.8525\n",
      "Epoch 11/200\n",
      "320000/320000 [==============================] - 13s 41us/step - loss: 0.2909 - acc: 0.8566 - val_loss: 0.2924 - val_acc: 0.8544\n",
      "Epoch 12/200\n",
      "320000/320000 [==============================] - 13s 39us/step - loss: 0.2860 - acc: 0.8601 - val_loss: 0.2899 - val_acc: 0.8550\n",
      "Epoch 13/200\n",
      "320000/320000 [==============================] - 14s 44us/step - loss: 0.2824 - acc: 0.8619 - val_loss: 0.2860 - val_acc: 0.8584\n",
      "Epoch 14/200\n",
      "320000/320000 [==============================] - 13s 40us/step - loss: 0.2784 - acc: 0.8644 - val_loss: 0.2833 - val_acc: 0.8588\n",
      "Epoch 15/200\n",
      "320000/320000 [==============================] - 13s 40us/step - loss: 0.2745 - acc: 0.8670 - val_loss: 0.2828 - val_acc: 0.8614\n",
      "Epoch 16/200\n",
      "320000/320000 [==============================] - 13s 41us/step - loss: 0.2721 - acc: 0.8690 - val_loss: 0.2794 - val_acc: 0.8621\n",
      "Epoch 17/200\n",
      "320000/320000 [==============================] - 13s 39us/step - loss: 0.2683 - acc: 0.8711 - val_loss: 0.2757 - val_acc: 0.8659\n",
      "Epoch 18/200\n",
      "320000/320000 [==============================] - 13s 40us/step - loss: 0.2649 - acc: 0.8728 - val_loss: 0.2771 - val_acc: 0.8652\n",
      "Epoch 19/200\n",
      "320000/320000 [==============================] - 13s 39us/step - loss: 0.2621 - acc: 0.8746 - val_loss: 0.2726 - val_acc: 0.8676\n",
      "Epoch 20/200\n",
      "320000/320000 [==============================] - 13s 41us/step - loss: 0.2597 - acc: 0.8763 - val_loss: 0.2724 - val_acc: 0.8676\n",
      "Epoch 21/200\n",
      "320000/320000 [==============================] - 13s 40us/step - loss: 0.2572 - acc: 0.8777 - val_loss: 0.2709 - val_acc: 0.8677\n",
      "Epoch 22/200\n",
      "320000/320000 [==============================] - 13s 40us/step - loss: 0.2545 - acc: 0.8792 - val_loss: 0.2733 - val_acc: 0.8672\n",
      "Epoch 23/200\n",
      "320000/320000 [==============================] - 13s 40us/step - loss: 0.2525 - acc: 0.8803 - val_loss: 0.2688 - val_acc: 0.8704\n",
      "Epoch 24/200\n",
      "320000/320000 [==============================] - 13s 39us/step - loss: 0.2504 - acc: 0.8816 - val_loss: 0.2694 - val_acc: 0.8688\n",
      "Epoch 25/200\n",
      "320000/320000 [==============================] - 13s 40us/step - loss: 0.2482 - acc: 0.8827 - val_loss: 0.2649 - val_acc: 0.8712\n",
      "Epoch 26/200\n",
      "320000/320000 [==============================] - 13s 41us/step - loss: 0.2464 - acc: 0.8842 - val_loss: 0.2658 - val_acc: 0.8716\n",
      "Epoch 27/200\n",
      "320000/320000 [==============================] - 14s 45us/step - loss: 0.2448 - acc: 0.8851 - val_loss: 0.2672 - val_acc: 0.8717\n",
      "Epoch 28/200\n",
      "320000/320000 [==============================] - 13s 41us/step - loss: 0.2441 - acc: 0.8862 - val_loss: 0.2641 - val_acc: 0.8738\n",
      "Epoch 29/200\n",
      "320000/320000 [==============================] - 13s 41us/step - loss: 0.2428 - acc: 0.8864 - val_loss: 0.2609 - val_acc: 0.8752\n",
      "Epoch 30/200\n",
      "320000/320000 [==============================] - 13s 39us/step - loss: 0.2408 - acc: 0.8880 - val_loss: 0.2627 - val_acc: 0.8748\n",
      "Epoch 31/200\n",
      "320000/320000 [==============================] - 14s 43us/step - loss: 0.2391 - acc: 0.8882 - val_loss: 0.2632 - val_acc: 0.8734\n",
      "Epoch 32/200\n",
      "320000/320000 [==============================] - 12s 38us/step - loss: 0.2375 - acc: 0.8895 - val_loss: 0.2614 - val_acc: 0.8752\n",
      "Epoch 33/200\n",
      "320000/320000 [==============================] - 13s 39us/step - loss: 0.2377 - acc: 0.8897 - val_loss: 0.2618 - val_acc: 0.8742\n",
      "Epoch 34/200\n",
      "320000/320000 [==============================] - 13s 40us/step - loss: 0.2366 - acc: 0.8905 - val_loss: 0.2648 - val_acc: 0.8734\n",
      "Epoch 35/200\n",
      "320000/320000 [==============================] - 12s 39us/step - loss: 0.2346 - acc: 0.8913 - val_loss: 0.2612 - val_acc: 0.8768\n",
      "Epoch 36/200\n",
      "320000/320000 [==============================] - 12s 39us/step - loss: 0.2326 - acc: 0.8929 - val_loss: 0.2639 - val_acc: 0.8735\n",
      "Epoch 37/200\n",
      "320000/320000 [==============================] - 12s 39us/step - loss: 0.2314 - acc: 0.8930 - val_loss: 0.2594 - val_acc: 0.8768\n",
      "Epoch 38/200\n",
      "320000/320000 [==============================] - 13s 41us/step - loss: 0.2305 - acc: 0.8941 - val_loss: 0.2603 - val_acc: 0.8754\n",
      "Epoch 39/200\n",
      "320000/320000 [==============================] - 13s 41us/step - loss: 0.2296 - acc: 0.8950 - val_loss: 0.2612 - val_acc: 0.8771\n",
      "Epoch 40/200\n",
      "320000/320000 [==============================] - 13s 39us/step - loss: 0.2274 - acc: 0.8957 - val_loss: 0.2593 - val_acc: 0.8773\n",
      "Epoch 41/200\n",
      "320000/320000 [==============================] - 13s 39us/step - loss: 0.2279 - acc: 0.8956 - val_loss: 0.2582 - val_acc: 0.8783\n",
      "Epoch 42/200\n",
      "320000/320000 [==============================] - 13s 41us/step - loss: 0.2262 - acc: 0.8960 - val_loss: 0.2598 - val_acc: 0.8777\n",
      "Epoch 43/200\n",
      "320000/320000 [==============================] - 13s 39us/step - loss: 0.2261 - acc: 0.8969 - val_loss: 0.2583 - val_acc: 0.8775\n",
      "Epoch 44/200\n",
      "320000/320000 [==============================] - 13s 40us/step - loss: 0.2238 - acc: 0.8978 - val_loss: 0.2573 - val_acc: 0.8781\n",
      "Epoch 45/200\n",
      "320000/320000 [==============================] - 12s 39us/step - loss: 0.2237 - acc: 0.8981 - val_loss: 0.2584 - val_acc: 0.8777\n",
      "Epoch 46/200\n",
      "320000/320000 [==============================] - 12s 39us/step - loss: 0.2227 - acc: 0.8988 - val_loss: 0.2574 - val_acc: 0.8783\n",
      "Epoch 47/200\n",
      "320000/320000 [==============================] - 12s 39us/step - loss: 0.2216 - acc: 0.8996 - val_loss: 0.2598 - val_acc: 0.8777\n",
      "Epoch 48/200\n",
      "320000/320000 [==============================] - 13s 40us/step - loss: 0.2206 - acc: 0.8998 - val_loss: 0.2601 - val_acc: 0.8773\n",
      "Epoch 49/200\n",
      "320000/320000 [==============================] - 13s 42us/step - loss: 0.2195 - acc: 0.9010 - val_loss: 0.2573 - val_acc: 0.8796\n",
      "Epoch 50/200\n",
      "320000/320000 [==============================] - 14s 42us/step - loss: 0.2189 - acc: 0.9010 - val_loss: 0.2582 - val_acc: 0.8781\n",
      "Epoch 51/200\n",
      "320000/320000 [==============================] - 12s 38us/step - loss: 0.2185 - acc: 0.9013 - val_loss: 0.2573 - val_acc: 0.8782\n",
      "Epoch 52/200\n",
      "320000/320000 [==============================] - 13s 40us/step - loss: 0.2170 - acc: 0.9022 - val_loss: 0.2591 - val_acc: 0.8800\n",
      "Epoch 53/200\n",
      "320000/320000 [==============================] - 14s 43us/step - loss: 0.2154 - acc: 0.9024 - val_loss: 0.2592 - val_acc: 0.8781\n",
      "Epoch 54/200\n",
      "320000/320000 [==============================] - 13s 42us/step - loss: 0.2156 - acc: 0.9029 - val_loss: 0.2561 - val_acc: 0.8805\n",
      "Epoch 55/200\n",
      "320000/320000 [==============================] - 15s 45us/step - loss: 0.2136 - acc: 0.9037 - val_loss: 0.2568 - val_acc: 0.8806\n",
      "Epoch 56/200\n",
      "320000/320000 [==============================] - 13s 42us/step - loss: 0.2144 - acc: 0.9036 - val_loss: 0.2577 - val_acc: 0.8787\n",
      "Epoch 57/200\n",
      "320000/320000 [==============================] - 13s 42us/step - loss: 0.2130 - acc: 0.9049 - val_loss: 0.2554 - val_acc: 0.8802\n",
      "Epoch 58/200\n",
      "320000/320000 [==============================] - 13s 40us/step - loss: 0.2129 - acc: 0.9046 - val_loss: 0.2609 - val_acc: 0.8794\n",
      "Epoch 59/200\n",
      "320000/320000 [==============================] - 13s 40us/step - loss: 0.2136 - acc: 0.9041 - val_loss: 0.2599 - val_acc: 0.8799\n",
      "\n",
      "--- Learning curve of model training ---\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 600x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Check against test data ---\n",
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Error when checking input: expected dense_1_input to have shape (147,) but got array with shape (149,)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-9ccace15a7ce>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0;31m#8 by 5, 9 by 5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_m\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nAccuracy on test data: %0.2f\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/models.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, x, y, batch_size, verbose, sample_weight, steps)\u001b[0m\n\u001b[1;32m   1002\u001b[0m                                    \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1003\u001b[0m                                    \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1004\u001b[0;31m                                    steps=steps)\n\u001b[0m\u001b[1;32m   1005\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1006\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, x, y, batch_size, verbose, sample_weight, steps)\u001b[0m\n\u001b[1;32m   1766\u001b[0m             \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1767\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1768\u001b[0;31m             batch_size=batch_size)\n\u001b[0m\u001b[1;32m   1769\u001b[0m         \u001b[0;31m# Prepare inputs, delegate logic to `_test_loop`.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1770\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muses_learning_phase\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearning_phase\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m   1474\u001b[0m                                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_feed_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1475\u001b[0m                                     \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1476\u001b[0;31m                                     exception_prefix='input')\n\u001b[0m\u001b[1;32m   1477\u001b[0m         y = _standardize_input_data(y, self._feed_output_names,\n\u001b[1;32m   1478\u001b[0m                                     \u001b[0moutput_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    121\u001b[0m                             \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m                             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' but got array with shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m                             str(data_shape))\n\u001b[0m\u001b[1;32m    124\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking input: expected dense_1_input to have shape (147,) but got array with shape (149,)"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense\n",
    "from keras.optimizers import SGD\n",
    "from keras.layers import Conv1D\n",
    "from keras.layers import MaxPooling1D\n",
    "from keras.layers import GlobalAveragePooling1D\n",
    "from keras.layers import Dropout\n",
    "from matplotlib import pyplot as plt\n",
    "from keras import optimizers\n",
    "import keras\n",
    "\n",
    "numFeatures = len(X[0])\n",
    "print(\"Num of Features: \", numFeatures)\n",
    "\n",
    "\n",
    "DropoutAmount = 0.3\n",
    "NodesPerLayer= int((DropoutAmount*numFeatures)) + numFeatures + 1\n",
    "\n",
    "model_m = Sequential()\n",
    "model_m.add(Dense(NodesPerLayer, activation='relu', input_shape=(numFeatures,)))\n",
    "model_m.add(Dense(NodesPerLayer, activation='relu'))\n",
    "model_m.add(Dropout(DropoutAmount))\n",
    "model_m.add(Dense(NodesPerLayer, activation='relu'))\n",
    "model_m.add(Dropout(DropoutAmount))\n",
    "model_m.add(Dense(NodesPerLayer, activation='relu'))\n",
    "model_m.add(Dropout(DropoutAmount))\n",
    "model_m.add(Dense(NodesPerLayer, activation='relu'))\n",
    "model_m.add(Dropout(DropoutAmount))\n",
    "model_m.add(Dense(NodesPerLayer, activation='relu'))\n",
    "model_m.add(Dropout(DropoutAmount))\n",
    "model_m.add(Dense(2, activation='softmax'))\n",
    "print(model_m.summary())\n",
    "\n",
    "\n",
    "callbacks_list = [\n",
    "    keras.callbacks.ModelCheckpoint(\n",
    "        filepath='best_model.{epoch:02d}-{val_loss:.2f}.h5',\n",
    "        monitor='val_loss', save_best_only=True),\n",
    "    keras.callbacks.EarlyStopping(monitor='acc', patience=2)\n",
    "]\n",
    "\n",
    "opt = optimizers.Adam(lr=0.002, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "\n",
    "model_m.compile(loss='binary_crossentropy',\n",
    "                optimizer=opt, metrics=['accuracy'])\n",
    "\n",
    "BATCH_SIZE = 475\n",
    "EPOCHS = 200\n",
    "\n",
    "history = model_m.fit(x_train,\n",
    "                      y_train,\n",
    "                      batch_size=BATCH_SIZE,\n",
    "                      epochs=EPOCHS,\n",
    "                      callbacks=callbacks_list,\n",
    "                      validation_split=0.2,\n",
    "                      verbose=1)\n",
    "\n",
    "\n",
    "print(\"\\n--- Learning curve of model training ---\\n\")\n",
    "\n",
    "# summarize history for accuracy and loss\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.plot(history.history['acc'], \"g--\", label=\"Accuracy of training data\")\n",
    "plt.plot(history.history['val_acc'], \"g\", label=\"Accuracy of validation data\")\n",
    "plt.plot(history.history['loss'], \"r--\", label=\"Loss of training data\")\n",
    "plt.plot(history.history['val_loss'], \"r\", label=\"Loss of validation data\")\n",
    "plt.title('Model Accuracy and Loss')\n",
    "plt.ylabel('Accuracy and Loss')\n",
    "plt.xlabel('Training Epoch')\n",
    "plt.ylim(0)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n--- Check against test data ---\\n\")\n",
    "\n",
    "\n",
    "#8 by 5, 9 by 5\n",
    "\n",
    "score = model_m.evaluate(x_test, y_test, verbose=1)\n",
    "\n",
    "print(\"\\nAccuracy on test data: %0.2f\" % score[1])\n",
    "print(\"\\nLoss on test data: %0.2f\" % score[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline:  0.66529\n"
     ]
    }
   ],
   "source": [
    "total = len(x_test)\n",
    "suspNum = 0\n",
    "\n",
    "for instance in y_test:\n",
    "    if instance[1] == 1:\n",
    "        suspNum += 1\n",
    "\n",
    "nonSUSPNum = total - suspNum\n",
    "print(\"Baseline: \", nonSUSPNum/total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "147\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_15 (Dense)             (None, 192)               28416     \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 192)               37056     \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 192)               0         \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 192)               37056     \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 192)               0         \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 192)               37056     \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 192)               0         \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 192)               37056     \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (None, 192)               0         \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 192)               37056     \n",
      "_________________________________________________________________\n",
      "dropout_15 (Dropout)         (None, 192)               0         \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 2)                 386       \n",
      "=================================================================\n",
      "Total params: 214,082\n",
      "Trainable params: 214,082\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "100000/100000 [==============================] - 5s 51us/step\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-86b132e8f1e8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_m\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m \u001b[0mtn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfusion_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"True Positives: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 4)"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import time\n",
    "import keras\n",
    "\n",
    "modelName = \"5-5_and_4-5.h5\"\n",
    "\n",
    "data =  np.loadtxt('../data/maybes_random_canon_r5_c5.csv',dtype = float, delimiter = ',')\n",
    "#names = data[0]\n",
    "#data = data[1:]\n",
    "\n",
    "#Sets class data to y\n",
    "y_test = data[:,-1]\n",
    "#Choosing which features to include in X data Must be same as trained above\n",
    "x_test = data[:,5:-1]\n",
    "\n",
    "y_test = keras.utils.to_categorical(y_test) \n",
    "\n",
    "\n",
    "model_m = load_model(modelName)\n",
    "model_m.summary()\n",
    "\n",
    "first = time.time()\n",
    "\n",
    "score = model_m.evaluate(x_test, y_test, verbose=1)\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "total = end - first\n",
    "\n",
    "y_pred = model_m.predict(x_test)\n",
    "\n",
    "tn, fp, fn, tp = confusion_matrix(y_test.argmax(axis=0), y_pred.argmax(axis=0)).ravel()\n",
    "\n",
    "print(\"True Positives: \", tp)\n",
    "print(\"True Negatives: \", tn)\n",
    "print(\"False Positives: \", fp)\n",
    "print(\"False Negatives: \", fn)\n",
    "\n",
    "\n",
    "suspNum = 0\n",
    "\n",
    "for instance in y_test:\n",
    "    if instance[1] == 1:\n",
    "        suspNum += 1\n",
    "\n",
    "print(\"\\nRecall: %0.3f\" % (tp/(tp+fn)))\n",
    "print(\"Precision: %0.3f\" % (tp/(tp+fp)))\n",
    "\n",
    "print(\"Accuracy on test data: %0.3f\" % score[1])\n",
    "print(\"\\nLoss on test data: %0.3f\" % score[0])\n",
    "\n",
    "print(\"Total Time: \", total)\n",
    "print(\"Time Per: \", total/len(X))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline:  99935\n"
     ]
    }
   ],
   "source": [
    "total = len(x_test)\n",
    "suspNum = 0\n",
    "\n",
    "for instance in y_test:\n",
    "    if instance[1] == 1:\n",
    "        suspNum += 1\n",
    "\n",
    "nonSUSPNum = total - suspNum\n",
    "print(\"Baseline: \", suspNum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1 ... 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "print(y_test.argmax(axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " ...\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "print(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
